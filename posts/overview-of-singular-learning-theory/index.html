<!DOCTYPE html><html lang="en" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Overview of Singular Learning Theory" /><meta property="og:locale" content="en" /><meta name="description" content="Singular Learning Theory (SLT), pioneered by Sumio Watanabe, acknowledges the presence of singularities in statistical models. provides a foundation grounded in Algebraic Geometry to work with singularities. clarifies foundational results in probability and statistics when regularity assumptions are violated. studies how properties of model singularities and other algebro-geometric properties affect properties of statistical models." /><meta property="og:description" content="Singular Learning Theory (SLT), pioneered by Sumio Watanabe, acknowledges the presence of singularities in statistical models. provides a foundation grounded in Algebraic Geometry to work with singularities. clarifies foundational results in probability and statistics when regularity assumptions are violated. studies how properties of model singularities and other algebro-geometric properties affect properties of statistical models." /><link rel="canonical" href="https://edmundlth.github.io/posts/overview-of-singular-learning-theory/" /><meta property="og:url" content="https://edmundlth.github.io/posts/overview-of-singular-learning-theory/" /><meta property="og:site_name" content="Probably Singular" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-11-15T00:00:00+11:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Overview of Singular Learning Theory" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-12-05T22:09:57+11:00","datePublished":"2021-11-15T00:00:00+11:00","description":"Singular Learning Theory (SLT), pioneered by Sumio Watanabe, acknowledges the presence of singularities in statistical models. provides a foundation grounded in Algebraic Geometry to work with singularities. clarifies foundational results in probability and statistics when regularity assumptions are violated. studies how properties of model singularities and other algebro-geometric properties affect properties of statistical models.","headline":"Overview of Singular Learning Theory","mainEntityOfPage":{"@type":"WebPage","@id":"https://edmundlth.github.io/posts/overview-of-singular-learning-theory/"},"url":"https://edmundlth.github.io/posts/overview-of-singular-learning-theory/"}</script><title>Overview of Singular Learning Theory | Probably Singular</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Probably Singular"><meta name="application-name" content="Probably Singular"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://d3awt3adt4koyi.cloudfront.net/images/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Probably Singular</a></div><div class="site-subtitle font-italic">Mathematics and Machine Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/edmundlth" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['edmundlth95','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Overview of Singular Learning Theory</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <script src="/assets/js/sidenote.js"></script><div id="latex-macros" style="display:none"> $$ \newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\P}{\mathbb{P}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\V}{\mathbb{V}} \newcommand{\E}{\mathbb{E}} \newcommand{\A}{\mathbb{A}} \newcommand{\F}{\mathbb{F}} \newcommand{\L}{\mathcal{L}} \newcommand{\sgn}{\mathrm{sgn}} \newcommand{\brac}[1]{\left ( #1 \right)} \newcommand{\set}[1]{\left \{ #1 \right \}} \newcommand{\st}{\, \mid \,} \newcommand{\wh}{\,|\,} \newcommand{\abrac}[1]{\left \langle #1 \right \rangle} \newcommand{\sqbrac}[1]{\left [ #1 \right]} $$</div><h1 data-toc-skip>Overview of Singular Learning Theory</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Edmund Lau </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Mon, Nov 15, 2021, 12:00 AM +1100" >Nov 15, 2021<i class="unloaded">2021-11-15T00:00:00+11:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Dec 5, 2021, 10:09 PM +1100" >Dec 5, 2021<i class="unloaded">2021-12-05T22:09:57+11:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2067 words">11 min read</span></div></div><div class="post-content"><p>Singular Learning Theory (SLT), pioneered by Sumio Watanabe,</p><ul><li>acknowledges the presence of singularities in statistical models.<li>provides a foundation grounded in Algebraic Geometry to work with singularities.<li>clarifies foundational results in probability and statistics when regularity assumptions are violated.<li>studies how properties of model singularities and other algebro-geometric properties affect properties of statistical models.</ul><p><strong>Table of Content</strong></p><ul><li><a href="#the-task-of-learning">The Task of Learning</a><li><a href="#the-mathematics-of-learning">The Mathematics of Learning</a><li><a href="#kullback-leibler-divergence">Kullback-Leibler Divergence</a><ul><li><a href="#generalisation-gap">Generalisation Gap</a><li><a href="#model-selection">Model Selection</a></ul><li><a href="#singular-learning-theory">Singular Learning Theory</a></ul><h1 id="the-task-of-learning">The Task of Learning</h1><p>The task of a statistical learning machine is to discover structures and properties of a data generating process $X$ with probability density $q(x)$ from a set of examples (training data) $D_N = \set{X_1, \dots, X_N}$ identically and independently drawn from the process $X_i \sim q(x)$.</p><p>Indeed, in hopes of recovering the <strong>truth</strong> $q$ itself, we define a <strong>statistical model</strong>, which a family of probability distributions $p(x \mid w)$, parametrised by $d$-dimensional real parameters $w \in W \subset \R^d$ endowed with a <strong>prior</strong> probability density $\varphi(w)$. Given such <strong>model-truth-prior triplet</strong>, a learning machine</p>\[D_N \mapsto p(x|\hat{w})\]<p>follows specified algorithmic procedure to search for its best guess $\hat{w}$ or a family of guesses based on information it can glean from examples in $D_N$, so that they can be used to approximate $q(x)$.</p><p>Maximum Likelihood Estimation (MLE) is a well-known example of such a procedure where a single $\hat{w}$ is found by maximising the average log-likelihood function $L_N(w) = \frac{1}{N}\sum_{i} \log p(X_i \mid w)$ and $q$ is approximated by $p(x \mid \hat{w})$. One can also first apply Bayes Theorem to obtain a posterior distribution</p>\[p(w\|D_N) = \frac{e^{NL_N(w)} \varphi(w)}{\int_W e^{NL_N(w)} \varphi(w)dw}\]<p>and set $\hat{w}$ to be its maximum, a.k.a. maximum a posteriori (MAP) estimate. Of particular theoretical significance is the Bayesian estimate, where no single $\hat{w}$ is trusted, instead, the truth $q$ is estimated by the posterior average over $w$, i.e.</p>\[p^*(x) = \int_W p(x \mid w) p(w \mid D_N) dw.\]<p>As a concrete example of contemporary interest and to illustrate the scale at which modern big data and big models, the Generative Pre-trained Transformer 3 (GPT-3) is a deep neural network model with $d \sim 175$ billion parameters, trained using $N &gt;$ 400 billion natural language tokens scraped from the Web. The put emphasis on the algorithmic or “machine” aspect of learning, GPT-3 is trained with a variant of the workhorse of modern machine learning: Stochastic Gradient Descent, an iterative algorithm where a better $\hat{w}$ is proposed by taking a small step in the opposite direction of the gradient $\nabla_w \L(\hat{w}_{\text{old}})$ of a loss function $\L$. The efficient computation of gradient on a models with large $d$ and $N$ with backpropagation is itself a significant algorithmic breakthrough.</p><h1 id="the-mathematics-of-learning">The Mathematics of Learning</h1><p>Here we sketch several central mathematical objects that arise from the study of statistical learning machines described above and briefly discuss how singularities arise to necessitate the need for methods from algebraic geometry.</p><h1 id="kullback-leibler-divergence">Kullback-Leibler Divergence</h1><p>First, a quantity is needed to represent how well each probability density $p(x|w)$ in the model approximate the truth $q(x)$. For this, we use the Kullback-Leibler Divergence or relative entropy as established in information theory</p>\[K(w) := D_{KL}(q \|\| p) = \int q(x) \log \frac{q(x)}{p(x\|w)} dx. \label{kl-div}\]<p>It is a non-negative quantity $K(w) \geq 0$ for any $w$ and a density $p(x|\hat{w}) = q(x)$ almost everywhere if and only if $K(\hat{w}) = 0$. Put another way, $K(w) = \E_X[\log q(x) - \log p(x|w)]$, it is the <strong>generalisation error</strong> as it expresses the expected difference between (log of) the truth $q$ with a selected model $p(x|w)$. When the expection $\E_X$ is estimated using training data, we get the empirical version of $K$</p>\[K_N(w) := \frac{1}{N} \sum_{i = 1}^N \log \frac{q(X_i)}{p(X_i\|w)}\]<p>also known as the training error. Since, $\E_X[K_N(w)] = K(w)$, as the size $N$ of training data increases, $K_N(w)$ converges pointwise almost surely to $K(w)$ by law of large numbers. Observe that, the quantity above can be rewritten as</p>\[K_N(w) = -L_N(w) + \frac{1}{N}\sum_{i = 1}^N \log q(X_i)\]<p>where the second term is the empirical version of the entropy $S(q)$ of $q$. Since only the first term has $w$ dependence, maximising the log-likelihood $L_N(w)$ – as per MLE – minimises $K_N(w)$. Unfortunately, as Watanabe pointed out in \cite{Watanabe2009-mg}, minimising $K_N(w)$ does not translate minimising $K(w)$ even with $N \to \infty$ since the following limiting operations do not commute</p>\[\E_X[\min_w K_N(w)] \neq \min_w \E_X[K_N(w)] = \min_w K(w).\]<p>Therein lies the tragedy and opportunity of statistical learning theory. The hope for a single best estimation method is dashed but we are promised a rich theory where we shall study the various mode of convergence $K_N \to K$ in function space of differing topology and their correspondence to different learning machines.</p><h2 id="generalisation-gap">Generalisation Gap</h2><p>This naturally leads to the study of generalisation gap $K_N - K$. Since this quantity is a measurable function of the data set $D_N$, i.e. a random variable, we asked for uncertainty quantification and also for its rate of decay as $N \to \infty$ which quantifies “data efficiency”: how large a data set do we need to collect before we are confident with our theoretical guarantees.</p><p>Observe that, for $w$ where $K(w) &gt; 0$, $K_N(w)$ has finite strictly positive variance, $\sigma^2 &gt; 0$, we can invoke central limit theorem to get the following pointwise convergence in law to a normal distribution</p>\[\sqrt{N}\brac{K_N(w) - K(w)} \xrightarrow{d} N(0, \sigma^2). \label{eq:clt-empirical}\]<p>Unfortunately, it is not estimates far from $q$ that interest us, nor does pointwise convergence reveals much about the behaviour of the generalisation gap in the neighbourhood of $\set{w \wh K(w) = 0}$. As our main interest is to approximate the truth $q$, behaviour around neighbourhood of $\set{K(w) = 0}$ constitute the main theme of our study.</p><p>Here’s where we see that the <strong>geometry</strong> of the set of true parameters $W_0 = \set{w \in W \wh K(w) = 0}$ become important. Aside from the case of \emph{regular statistical models} where $W_0$ is a singleton set, $K(w) = 0$ forms an analytic variety in $\R^d$ in the general case of \emph{singular models}.</p><p>%TODO: Picture of singularity in K = 0</p><h2 id="model-selection">Model Selection</h2><p>Another question of great theoretical and practical importance is: How do we determine the relative merits of one model over another? That is, with only training samples $D_N$ and no access to the truth $q$, how do we know that a model $p_1(x|w)$ is better or worse than another $p_2(x|w)$? This can be quantified using <strong>model evidence</strong>,</p>\[Z_N = \int_W \prod_{i = 1}^N p(X_i\|w) \varphi(w) dw = \int_W e^{N L_N(w)} \varphi(w) dw\]<p>which is marginalises $w$, giving the probability of observing the training samples $X_i$ in $D_N$ under the model $p(x|w)$. It can be shown that this quantity is controlled by the following integral of Laplace form involving the generalisation error $K(w)$. <em>If</em> there is a unique $\hat{w}$ that minimises $K$, $K(\hat{w}) = 0$, and that the Hessian $\nabla^2K(w)$ or equivalently the Fisher information matrix $I(w)<em>{ij} = - \E_X[\p</em>{w_i}\p_{w_j}\log p(X|w)]$ has full rank at $\hat{w}$, we can obtain asymptotic expansion as $n \to \infty$ using Laplace’s method</p>\[\int_W e^{-n K(w)} \varphi(w) dw \sim \sqrt{\frac{2\pi}{\det \brac{n\nabla^2K(\hat{w})}}}\varphi(\hat{w}) = O\brac{n^{-\frac{d}{2}}} \label{eq:laplace}\]<p>where $d$ is dimension of the parameter space $W \subset \R^d$. That the dominant order is $\sim n^{-d/2}$ give rise to the famous Bayesian Information Criterion (BIC).\</p><p>However, here again we are foiled by the possibility that $K(w) = 0$ might not be a singleton set, but an analytic variety with complicated geometry blocking the use of Laplace method for approximating the integral in its neighbourhood.</p><h1 id="singular-learning-theory">Singular Learning Theory</h1><p>As we observed above, classical results in elementary probability and statistics are frequently frustrated when the statistical model violates the following regularity conditions:</p><ul><li>map from parameter to density $w \mapsto p(x|w)$ is one-to-one.<li>the Fisher information matrix $I(w)$ of the model is positive definite for all $w \in W$.</ul><p>Statistical models that violates the above assumptions are called (strictly) singular models. With singular models, lots of day-to-day statistical intuitions, tools and results no longer apply, for instance</p><ul><li>Cramer-Rao bound does not hold.<li>Maximum-likelihood estimator is not longer asymptotically Gaussian.<li>BIC and AIC (Akaike Information Criterion) are no longer approximations of what they claim to approximate.</ul><p>Worse still, in machine learning and many real world applications of statistics, singular models are the norm, not the exception [Watanabe2007]. In light of this, a new foundation for singular statistical model is needed and it hinges on the following remarkable result from Algebraic Geometry.</p><p><strong>Theorem (Hironaka’s resolution of singularities)</strong>[Watanabe2009]</p><p>Let $f: W \to \R$ be a non-constant real analytic function from a open neighbourhood of $W$ of the origin in $\R^d$ with $f(0) = 0$. Then there exist a <strong>resolution map</strong> $g: U \to W$ such that</p><ul><li>$U$ is a $d$-dimensional real analytic manifold.<li>$g$ is real analytic.<li>$g$ is a proper map.<li>with $W_0 = \set{w \in W \wh f(w) = 0}$ and $U_0 = g^{-1}(W_0)$, $g: U \setminus U_0 \to W \setminus W_0$ is an analytic isomorphism.<li>if $p \in U_0$, there is a local chart $u = (u_1, \dots, u_d)$ with $p$ as the origin such that $f(g(u))$ and $g’(u)$ are of the form<br /> \(f(g(u)) = S u_1^{k_1}u_2^{k_2}\dots u_{d}^{k_d} \\ g'(u) = b(u) u_1^{h_1}u_2^{h_2} \dots u_{d}^{h_d}\) where $S \in \set{+1, -1}$ is a sign, $b(u) \neq 0$ is real analytic and $k_i, h_i$ are nonnegative integers.</ul><p>To illustrate how such a transformation might help with singular models, we note that, in the desingularised coordinates using the resolution map $g: U \to W$ above, we can extend the result to</p>\[\sqrt{N}\frac{K_N(g(u)) - K(g(u))}{\sqrt{K(g(u))}} \xrightarrow{d} \xi(u)\]<p>where $\xi$ is a Gaussian process. And the asymptotic expansion above can now be done for singular models with</p>\[\log \int_W e^{-n K(w)} \varphi(w) dw \sim \lambda \log n - (m-1) \log \log n + o(\log \log n)\]<p>where $\lambda$, instead of $d/2$ in regular models, is a birational invariant of the variety $K(w) = 0$ known as reallog canonical threshold which can be characterised in many ways as summarised below:</p><p><strong>Proposition</strong> [Watanabe2009-mg, lin-phdthesis]</p><p>For a given statistical model $(p(x|\omega), q(x), \varphi(\omega))$, the following are equivalent definitions for its real log canonical threshold (RLCT), $\lambda$ and its order $\theta$.</p><ul><li><p><strong>Largest pole of zeta function of $K$</strong> Define the zeta function $\zeta: \C \to \C$ of $K$ as\footnote{$\zeta$ analytically continues to a meromorphic function with poles on the negative real axis.}</p>\[\zeta(z) = \int_\Omega K(w)^z \varphi(\omega)d\omega.\]<p>The RLCT $\lambda$ is the largest pole of $\zeta$ and $\theta$ the order of the pole at $\lambda$.</p><li><p><strong>Convergence rate of Laplace integral of $K$</strong> $(\lambda, \theta)$ governs the asymptotic behaviour as $n \to \infty$ of the Laplace integral\footnote{which is the deterministic version of the (normalised) evidence $Z^0<em>n = \int</em>\Omega \exp\brac{-nK_N(\omega)}\varphi(\omega) d\omega$. Note that the limiting variable $n$ is different from the number of training samples $N$. This is one place where inverse temperature $\beta$ can come in: set $n = \beta k$.}:</p>\[\int_\Omega \exp\brac{-nK(\omega)}\varphi(\omega) d\omega \stackrel{n \to \infty} \sim Cn^{-\lambda}\brac{\log n}^{\theta -1}\]<p>for some positive real constant $C$.</p><li><p><strong>Convergence rate of free energy</strong> Taking the negative logarithm of the previous asymptotic expression gives\footnote{the stochastic version translate as $F^0_n = \lambda \log n - (\theta -1) \log \log n + $ stochastic process of constant order. }</p>\[\log \int_\Omega \exp\brac{-nK(\omega)}\varphi(\omega) d\omega \stackrel{n \to \infty} \sim \lambda \log n - \brac{\theta -1} \log \log n + O(1).\]<li><p><strong>Asymptotic expansion of density of states near $W_0$</strong> The density of state \(v(t) = \int_\Omega \delta\brac{t - K(\omega)} \varphi(\omega) d\omega\) has asymptotic expansion as $t \to 0$ \(v(t) \sim C t^{\lambda -1} (- \log(t))^{\theta -1}\) for some positive real constant $C$.\footnote{lots of fixing and clarification needed…Mellin transform’s involved.}</p><li><p><strong>Volume codimension $W_0$</strong></p>\[\lambda = \lim_{t \to 0^+} \log_a \frac{V(at)}{V(t)}\]<p>where $1 \neq a &gt; 0$ and $V: \R_{\geq 0} \to \R_{\geq 0}$ is the volume measure of neighbourhoods of $W_0$</p>\[V(t) = \int_{K(w) &lt; t} \varphi(\omega) d\omega.\]<li><p><strong>From resolution of singularity</strong> Hironaka’s resolution of singularity for the real analytic function $K(\omega)$ gives us a proper birational map\footnote{obtained via recursive blow up.} $g: U \to \Omega$ such that in the neighbourhood of $\omega_0 \in W_0$, the zero set of $K$</p>\[\begin{align*} K(g(u) - \omega_0) &amp;= u^{2k} = u_1^{2k_1}u_2^{2k_2} \dots u_d^{2k_d}\\ g'(u) &amp;= b(u)u^h = b(u)u_1^{h_1}u_2^{h_2} \dots u_d^{h_d} \end{align*}\]<p>for some $u, k \in \N^d$ and analytic $b(u) \neq 0$. We then have</p>\[\lambda = \inf_{\omega \in W_0} \min_{1 \leq j \leq d}\frac{h_j + 1}{2k_j}\]<p>and $\theta$ is given by the number of times the above minimum is achieved. <span sidenote="">This deep result shows that $(\lambda, \theta) \in \Q \times \Z$. </span></p></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/singular-learning-theory/'>singular-learning-theory</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/singular-learning-theory/" class="post-tag no-text-decoration" >singular-learning-theory</a> <a href="/tags/real-log-canonical-threshold/" class="post-tag no-text-decoration" >real-log-canonical-threshold</a> <a href="/tags/statistics/" class="post-tag no-text-decoration" >statistics</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >machine-learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Overview of Singular Learning Theory - Probably Singular&url=https://edmundlth.github.io/posts/overview-of-singular-learning-theory/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Overview of Singular Learning Theory - Probably Singular&u=https://edmundlth.github.io/posts/overview-of-singular-learning-theory/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/statistical-learning-theory-via-a-coin-flipping-demon/">Statistical Learning Theory Via A Coin Flipping Demon</a><li><a href="/posts/singular-learning-theory-part-1/">Singular Learning Theory - Part 1</a><li><a href="/posts/hartshorne-algebraic-geometry-1.1/">Algebraic Geometry - Hartshorne 1.1</a><li><a href="/posts/introduction-to-grobner-basis/">Introduction to Grobner Basis</a><li><a href="/posts/overview-of-singular-learning-theory/">Overview of Singular Learning Theory</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/machine-learning/">machine-learning</a> <a class="post-tag" href="/tags/singular-learning-theory/">singular-learning-theory</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/seminar-notes/">seminar-notes</a> <a class="post-tag" href="/tags/algebraic-geometry/">algebraic-geometry</a> <a class="post-tag" href="/tags/grobner-basis/">grobner-basis</a> <a class="post-tag" href="/tags/hartshorne/">hartshorne</a> <a class="post-tag" href="/tags/real-log-canonical-threshold/">real-log-canonical-threshold</a></div></div></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/statistical-learning-theory-via-a-coin-flipping-demon/"><div class="card-body"> <span class="timeago small" >Jan 13<i class="unloaded">2022-01-13T00:00:00+11:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Statistical Learning Theory Via A Coin Flipping Demon</h3><div class="text-muted small"><p> “It is simplicity itself, so absurdly simple that an explanation is superfluous; and yet it may serve to define the limits of observation and of deduction." Sherlock Holmes, "The Sign...</p></div></div></a></div><div class="card"> <a href="/posts/ladder-to-free-energy-asymptotics/"><div class="card-body"> <span class="timeago small" >Feb 24<i class="unloaded">2022-02-24T00:00:00+11:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Ladder to Free Energy Asymptotics</h3><div class="text-muted small"><p></p></div></div></a></div><div class="card"> <a href="/posts/climbing-ladder-to-free-energy-asymptotics/"><div class="card-body"> <span class="timeago small" >Mar 17<i class="unloaded">2022-03-17T00:00:00+11:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Climbing Ladder to Free Energy Asymptotics</h3><div class="text-muted small"><p></p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/introduction-to-grobner-basis/" class="btn btn-outline-primary" prompt="Older"><p>Introduction to Grobner Basis</p></a> <a href="/posts/singular-learning-theory-part-1/" class="btn btn-outline-primary" prompt="Newer"><p>Singular Learning Theory - Part 1</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://edmundlth.github.io/posts/overview-of-singular-learning-theory/'; this.page.identifier = '/posts/overview-of-singular-learning-theory/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://probably-singular.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (modeToggle !== null) { modeToggle.addEventListener('click', reloadDisqus); window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/edmundlth">Edmund Lau</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/singular-learning-theory/">singular learning theory</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/seminar-notes/">seminar notes</a> <a class="post-tag" href="/tags/algebraic-geometry/">algebraic geometry</a> <a class="post-tag" href="/tags/grobner-basis/">grobner basis</a> <a class="post-tag" href="/tags/hartshorne/">hartshorne</a> <a class="post-tag" href="/tags/real-log-canonical-threshold/">real log canonical threshold</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
