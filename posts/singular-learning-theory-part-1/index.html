<!DOCTYPE html><html lang="en" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Singular Learning Theory - Part 1" /><meta property="og:locale" content="en" /><meta name="description" content="“We approached the case, you remember, with an absolutely blank mind, which is always an advantage. We had formed no theories. We were simply there to observe and to draw inferences from our observations.” Sherlock Holmes, &quot;The Adventure of The Cardboard Box&quot;" /><meta property="og:description" content="“We approached the case, you remember, with an absolutely blank mind, which is always an advantage. We had formed no theories. We were simply there to observe and to draw inferences from our observations.” Sherlock Holmes, &quot;The Adventure of The Cardboard Box&quot;" /><link rel="canonical" href="https://edmundlth.github.io/posts/singular-learning-theory-part-1/" /><meta property="og:url" content="https://edmundlth.github.io/posts/singular-learning-theory-part-1/" /><meta property="og:site_name" content="Probably Singular" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-12-28T00:00:00+11:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Singular Learning Theory - Part 1" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-02-09T15:07:51+11:00","datePublished":"2021-12-28T00:00:00+11:00","description":"“We approached the case, you remember, with an absolutely blank mind, which is always an advantage. We had formed no theories. We were simply there to observe and to draw inferences from our observations.” Sherlock Holmes, &quot;The Adventure of The Cardboard Box&quot;","headline":"Singular Learning Theory - Part 1","mainEntityOfPage":{"@type":"WebPage","@id":"https://edmundlth.github.io/posts/singular-learning-theory-part-1/"},"url":"https://edmundlth.github.io/posts/singular-learning-theory-part-1/"}</script><title>Singular Learning Theory - Part 1 | Probably Singular</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Probably Singular"><meta name="application-name" content="Probably Singular"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://d3awt3adt4koyi.cloudfront.net/images/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Probably Singular</a></div><div class="site-subtitle font-italic">Mathematics and Machine Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/edmundlth" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['edmundlth95','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Singular Learning Theory - Part 1</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <script src="/assets/js/sidenote.js"></script><div id="latex-macros" style="display:none"> $$ \newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\P}{\mathbb{P}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\V}{\mathbb{V}} \newcommand{\E}{\mathbb{E}} \newcommand{\A}{\mathbb{A}} \newcommand{\F}{\mathbb{F}} \newcommand{\L}{\mathcal{L}} \newcommand{\sgn}{\mathrm{sgn}} \newcommand{\brac}[1]{\left ( #1 \right)} \newcommand{\set}[1]{\left \{ #1 \right \}} \newcommand{\st}{\, \mid \,} \newcommand{\wh}{\,|\,} \newcommand{\abrac}[1]{\left \langle #1 \right \rangle} \newcommand{\sqbrac}[1]{\left [ #1 \right]} $$</div><h1 data-toc-skip>Singular Learning Theory - Part 1</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Edmund Lau </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Dec 28, 2021, 12:00 AM +1100" >Dec 28, 2021<i class="unloaded">2021-12-28T00:00:00+11:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Wed, Feb 9, 2022, 12:07 PM +0800" >Feb 9<i class="unloaded">2022-02-09T15:07:51+11:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3114 words">17 min read</span></div></div><div class="post-content"> <img data-proofer-ignore data-src="https://d3awt3adt4koyi.cloudfront.net/images/2021-12-28/algebraic-geometry-and-statistical-learning-theory-cover-image.jpg" class="preview-img" alt="image not found" width="500" ><div class="epi"><blockquote> “We approached the case, you remember, with an absolutely blank mind, which is always an advantage. We had formed no theories. We were simply there to observe and to draw inferences from our observations.” <cite> Sherlock Holmes, "The Adventure of The Cardboard Box" </cite></blockquote></div><p> This is intended as the first of a series of articles going through core texts for Singular Learning Theory (henceforth SLT) - "Algebraic Geometry and Statistical Learning Theory"(<a href="#ref-1">Watanabe, 2009</a>) and "Mathematical Theory of Bayesian Statistics" (<a href="#ref-2">Watanabe 2018</a>) both written by Sumio Watanabe. This is perhaps mainly my attempt at digesting the text<span sidenote=""> not for the first time </span> disguised as a series of lectures. The target audience I have in mind is anyone familiar with undergraduate level mathematics and is interested in theoretical / mathematical aspect of statistical learning and AI. That being said, the nature of the subject is such that it draws upon tools and concepts from a wide array of mathematical disciplines, spanning algebra to analysis. Some, like probability and statistics, are crucial in the sense that they are the objects of study. Some, like manifold theory, are only required to make sure that the mathematical objects we manipulate are well defined and cover a sufficient generality for the theory to be useful. Others, like algebraic geometry and Schwartz distribution theory, exports crucial theorems that we shall use to prove and understand the central results of SLT. Yet others, like statistical mechanics, are topics where we might find unexpected connections and possible cross-pollination. We shall introduce these topics in their own time when they come up naturally when we explore SLT. Our modest aim regarding these prerequisites is to understand them with sufficient depth to understand the proofs of various results in SLT and to at least understand their significance <span sidenote=""> like why they are needed and what happen when we can't borrow from them. Of course each of them are profound fields of study in their own right, and if time and energy permit, we shall delve beyond strictly necessary to see the wonder they contain.</span></p><ul><li><a href="#what-do-we-aspire-to-study">What do we aspire to study?</a><li><a href="#statistical-background">Statistical Background</a><ul><li><a href="#the-task-of-learning">The Task of Learning</a><li><a href="#model-truth-prior">Model-Truth-Prior</a><li><a href="#some-important-statistical-estimation-methods">Some Important Statistical Estimation Methods</a><li><a href="#kullback-leibler-divergence">Kullback-Leibler Divergence</a><li><a href="#theory-of-statistical-learning">Theory of Statistical Learning</a><li><a href="#fisher-information-matrix">Fisher Information Matrix</a></ul><li><a href="#singular-models">Singular Models</a><li><a href="#model-selection">Model Selection</a><li><a href="#misc">MISC</a><li><a href="#references">References</a></ul><h1 id="what-do-we-aspire-to-study">What do we aspire to study?</h1><p>Before making definitions and carving out our domain, let’s list a few informal ways we think and talk about our field of study <span sidenote=""> If nothing else, it will help with triangulating what we aspire to study, where the definitions and tools we shall develop are helpful and where there are not. </span>.</p><ul><li>We are studying systems that can learn from examples.<li>We are studying algorithms or physical devices that perceive the world and learn its patterns.<li>We are studying ways to reason and make decision with imperfect information and in the presence of randomness.<li>We are studying models of systems with randomness and how to find the best approximation for the particular part of the world under observation.<li>We are studying, or at least trying to study what we think is, intelligence. In particular, machine intelligence, of how machine can learn patterns and make intelligent decisions.</ul><details> <summary> Examples of objects of study that falls under these aspirations. </summary><ul><li> A device that implement the calculation $\sum_{i = 1}^6 f(i)/6$ can make decision on whether a given payout $f$ of a game using a six-sided dice is worth playing.<li> Perhaps a better, more intelligent, device / machine would actually uses observations on 10000 dice throw and implement $\sum_{i = 1}^6 f(i) \frac{\mathrm{freq}(i)}{10000}$ instead.<li> A statistician can use a sample of human height measurements $\set{h_1, \dots, h_n}$ and estimate the distribution of heights of comparable human population, or to make a good guess on the probability that a child would grow to be taller than 150cm.<li> The natural language model GPT-3 is a massive and complicated approximation to human languages.<li> <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far"> AlphaGo</a> is a machine intelligence that were able to learn from millions of generated examples of games of Go, and learn the pattern of winning strategies so well that it was able to defeat the reigning world champion.<li> Brains of human and other intelligent species seems to implement complicated and as yet poorly understood system that learn the pattern of their surroundings, able to triage tasks and resources and make decisions critical to the survival of its species.</ul></details><h1 id="statistical-background">Statistical Background</h1><h2 id="the-task-of-learning">The Task of Learning</h2><p>Let’s come back down to earth with some definitions. In typical statistical learning scenario, we are given a set of examples coming from a data generating process.</p><div class="def"> We are given a <span def=""> data set </span> $$ D_N = \set{X_1, \dots, X_N} $$ of $N \in \N$ <span def=""> training samples </span> consisting of independently and identically distributed random variables $X_i$ <span> i.e. i.i.d. samples</span> drawn from what we shall call the <span def=""> true distribution</span> specified by a probability density $q(x)$.</div><p>Unless otherwise specified, the random variables take values in a fixed Euclidean space $\R^n$. For clarity, we shall often have rather strong regularity assumptions on any probability distributions that occurs throughout the development of SLT. Not only do we assume that densities exist for probability distributions we study, we shall also assume at least continuity and likely analyticity<span sidenote=""> full set of conditions will be specified later on. This setting allow us to leverage heavy duty machineries from algebraic geometry to explicate salient features of learning machines before persuing further generality. Though this seems rather restrictive, it does cover a large class of distributions we encounter in practice like normal mixtures, neural networks, Bayesian networks and much more. </span></p><p>The truth $q$ is what a learning machine wants to access, but can only see the shadow of via the data set $D_N$. For our purpose, we shall formulate the task of a statistical learning machine as one that approximates the true distribution $q$ from the given set of training samples $D_N$.</p><div class="def"> A <span def=""> statistical inference </span> or a <span def=""> learning machine </span> is a measurable function taking any data set $D_N$ to probability distribution<span sidenote=""> again, we shall work with distribution on $\R^n$.</span> specified by $\hat{p}(x)$, $$ \begin{align*} \mathrm{LearningMachine}: \set{\text{data set of any size $N$}} &amp;\to \set{\text{probability distributions}} \\ D_N &amp;\mapsto \hat{p}(x). \end{align*} $$</div><p>Among the concerns of a statistical learning theory includes studying</p><ul><li>how to construct such a learning machine<li>how to obtain $\hat{p}$ efficiently<span sidenote=""> efficiency in terms of computation resources and size of data set.</span><li>and how well does $\hat{p}$ thus constructed approximates $q$.</ul><h2 id="model-truth-prior">Model-Truth-Prior</h2><p>A useful and general way of constructing and studying the approximation $\hat{p}$ to a given <em>truth</em> $q(x)$ is to first looking at a large family of distributions, have a way of measuring how well each of them approximates $q$, then come up with a procedure to find the best performing approximation.</p><div class="def"><p> A statistical <span def=""> model </span><span sidenote="">we might sometimes call this the space of hypothesis</span> is a family of probability distribution $p(x \mid w)$ parameterised by a subset of $d$-dimensional <span def=""> parameter space </span> $W \subset \R^d$ for some $d \in \N$. We assume that $p(x \mid w)$ at all $w \in W$ have the same support as the true distribution $q(x)$, $$ \overline{\set{x \in \R^n \wh p(x | w) &gt; 0}} = \overline{\set{x \in \R^n \wh q(x) &gt; 0}}. $$ Given a set of i.i.d. training samples $D_N = \set{X_1, \dots, X_N}$ as before, we call probability of observing $D_N$ for a particular parameter $$ L_N(w) := \P(X_1, \dots, X_N \mid w) = \prod_{i = 1}^N p(X_i \mid w) $$ the <span def=""> likelihood function</span>.</p><p> Taking a Bayesian perspective, we shall endow the parameter space $W$ with a <span def=""> prior probability distribution </span> with density $\varphi(w)$ so that, applying Bayes Theorem, we get the posterior distribution on $W$ $$ p(w \mid D_N) = \frac{L_N(w) \varphi(w)}{Z_N} $$ where the normalising constant in the denominator $$ Z_N := \int_W L_N(w) \varphi(w) dw = \int_W \prod_{i = 1}^N p(X_i \mid w) \varphi(w) dw $$ is an important quantity known as the <span def=""> model evidence</span>.</p></div><details open=""> <summary> (Quite) a few things to say about $Z_N$ </summary><p> This quantity comes with a few different names<span sidenote="">testament to its importance and ubiquity in mathematical sciences.</span>. In Bayesian statistics, it goes by the names "model evidence" or "marginal likelihood". In statistical mechanics, it goes by "partition function" or, when taken a log-transform, the "free energy", denoted as $$ F_N = -\log Z_N $$</p><p> To justify its name, we observe that $Z_N$ depends on the set of random variables $D_N$, hence it is itself a random variable. In fact, it is precisely the probability of observing the data $\set{X_1, \dots, X_N}$ given the model and prior.<span sidenote=""> Perhaps it is clearer in the basic form of Bayes Theorem $P(H|X) = P(X|H) P(H) / P(X)$ thinking of $H$ as a particular hypothesis in a class of hypotheses and $X$ as the observed data. The denominator $P(X) = \sum_{h \in \text{all hypothesis}} P(X|h)P(h)$ is the probability of observing $X$, $P(X)$, given the hypothesis class. </span> It is there for the likelihood of the given model-prior pair given the data, hence "model evidence". It is computed by marginalise out the model parameter $w$, hence "marginal likelihood".</p><p> We shall also consider a generalisation of the posterior distribution and model evidence where introduce a continuous parameter $\beta &gt; 0$ which we shall interpret as "inverse temperature" $\beta = \frac{1}{T}$: $$ \begin{align*} p(w \mid D_N) &amp;= \frac{\varphi(w) \prod_{i = 1}^N p(X_i \mid w)^\beta}{Z_N(\beta)}\\ Z_N(\beta) &amp;= \int_W \varphi(w) \prod_{i = 1}^N p(X_i \mid w)^\beta dw. \end{align*} $$ Note that<ul><li>$\beta = 1$ reduces down to the case of usual Bayes posterior distribution and evidence.<li>As $\beta \to \infty$, the posterior distribution converges the delta distribution concentrated at the maximum likelihood estimate of $w$, $\delta(w - \hat{w}_{ML})$, if it exist and is unique in the support of $\varphi(w)$. We can think of this case as taking the temperature down to zero and having the system considers only the ground state specified by the observed data.<li>As $\beta \to zero$, the posterior collapsed to the prior. This is the case where we have infinite temperature and observed data has no effect on the system exploring all allowed state at the frequency specified by the prior.</ul></p><p> To make the analogy to statistical mechanics even tighter, we can introduce the analog to the potential field by considering quantities normalised by the true probabilities of observed data $q(X_i)$. $$ \begin{align*} Z^0_N(\beta) &amp;= \frac{Z_N(\beta)}{\prod_{i = 1}^N q(X_i)^\beta} \\ F^0_N(\beta) &amp;= -\log Z^0_N(\beta) = F_N(\beta) - N\beta S_N \\ S_N &amp;= -\frac{1}{N} \sum_{i = 1}^N \log(q(X_i)) \end{align*} $$ where $S_N$ is the empirical entropy. "Empirical" because it is an empirical estimate of the entropy of the true distribution given by $$ S(q) = - \int q(x) \log q(x) dx = \E[-\log q(x)] = \E[S_N]. $$</p></details><h2 id="some-important-statistical-estimation-methods">Some Important Statistical Estimation Methods</h2><p>With the above objects defined, we can now specify a few ways to construct $\hat{p}$.</p><p>First, there is the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood</a> method that constructs an approximation to the truth $q$ by finding, among the model parameter space $W$, the parameter $w \in W$ that maximises the likelihood function. The assumption is that, since the data set $D_N$ is generated from $q(x)$, a good approximation of $q$ is the model $p(x \mid w)$ that have the highest likelihood to have generated the same data.</p><div class="def"><p> The <span def=""> Maximum Likelihood Estimator (ML) </span> is given by the model parameter that maximises the likelihood function. $$ \hat{w}_{ML} = \mathrm{argmax}_{w \in W} L_N(w) $$ It turns out that it is easier in practice for numerical reasons and more convenient for theoretical discussions to introduce the negative log-likelihood function $l_N(w) = -log L_N$ and the MLE is given by minimising $l_N(w)$, i.e. $\hat{w}_{MLE} = \mathrm{argmin}_{w \in W} l_N(w)$. With the MLE, the approximate distribution is given by the model at $\hat{w}_{MLE}$, $$ \hat{p}_{ML}(x) = p(x \mid \hat{w}_{ML}). $$</p></div><p>MLE is a common <em>point estimation</em> method. Meaning an objective function, the likelihood function in this case, is constructed and then an optimisation procedure is employed to find the optimum which gives the desired estimate. It is often assumed that the model space is parametrised in a such a way that the likelihood function has desirable functional properties suitable for optimisation such as differentiability, convexity or, like in the case of large neural networks, convenient form to compute gradient for <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>.</p><p>Bringing some Bayesian perspective into the picture, we can construct another point estimate by biasing the likelihood function with prior belief of what parameters $w \in W$ are <em>a priori</em> more likely to occur without having observe any data. This gives us the <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">Maximum a Posteriori</a> method.</p><div class="def"><p> The <span def=""> Maximum a Posteriori (MAP) </span> estimate is obtained by maximising the posterior distribution $$ \hat{w}_{MAP} = \mathrm{argmax}_{w \in W} p(w \mid D_N) = \mathrm{argmin}_{w \in W} l_N(w) - \log \varphi(w) $$ where $\varphi(w)$ is the prior distribution. The approximation to $q(x)$ is then given by $$ \hat{p}_{MAP}(x) = p(x \mid \hat{w}_{MAP}). $$</p></div><p>In extreme cases, if we believe<span sidenote="">on sound theoretical grounds or just sheer human pig-headedness</span> that the model parameters lies in a certain region and nowhere else, then even if the MLE method claim that the data is most likely to be generated by the model at $\hat{w}_{MLE}$, the MAP estimate might completely disregard that possibility.</p><p>Instead of considering one “best” parameter in the model, a fully Bayesian perspective will advocate for taking distributions at all parameters $w \in W$ into account, with the importance of each $p(x \mid w)$ weighted by their posterior probability.</p><div class="def"><p> The <span def=""> Bayes predictive distribution </span> for the approximation of $q(x)$ is given by $$ \hat{p}_{B} = p(x | D_N) = \E_{w}\sqbrac{p(x \mid w)} = \int_{W} p(x \mid w) p(w \mid D_N) dw $$ where $\E_w$ denote taking expectation with respect to the posterior.</p></div><p>Another method of constructing statistical estimation $\hat{p}$ perhaps rare in practice but holds theoretical importance is that of Gibbs estimation. With Gibbs estimation, every time<span sidenote="">I am a little unsure about whether we draw $\hat{w}$ once and for all or perform multiple draws as described.</span> one wish to estimate the probability of $X = x$, one starts by randomly drawing a parameter $\hat{w}$ from the posterior distribution $p(w \mid D_N)$ and answer with $p(X = x \mid \hat{w})$.</p><div class="def"><p> The <span def=""> Gibbs estimation</span> method is given by randomly drawing a parameter from the posterior distribution $$ \hat{w}_{gibbs} ~ p(w \mid D_N) $$ and give the approximation to $q(x)$ by $$ \hat{p}_{gibbs} = p(x \mid \hat{w}_{gibbs}). $$</p></div><h2 id="kullback-leibler-divergence">Kullback-Leibler Divergence</h2><p>Next come the problem of how to measure the success or failure of our approximation. We need a quantity that measures the difference between two density functions. Enter the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler</a> <a href="https://en.wikipedia.org/wiki/Divergence_(statistics)">divergence</a>.</p><div class="def"> The <span def=""> Kullback-Leibler Divergence </span> or KL-divergence between two probability density functions $q(x)$ and $p(x)$ on an open set $A \subset \R^n$ is given by $$ K(q \mid\mid p) := \int_A q(x) \log \frac{q(x)}{p(x)} dx. $$</div><details> <summary> KL-divergence on general probability measure </summary><p> <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL-divergence</a> can be defined in much greater generality between any two probability measures $P$ and $Q$ over the same measure space $(X, \sigma)$ by $$ K(Q \mid\mid P) := \int_X \log \frac{dQ}{dP} dQ $$ whenever the <a href="https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem#Radon%E2%80%93Nikodym_derivative">Radon-Nikodymn derivative</a> $\frac{dQ}{dP}$ exists <span sidenote="">i.e. $Q$ is absolutely continuous with respect to $P$.</span>. If there is another measure $\mu$ on $X$ for which $p = \frac{dP}{d\mu}$ and $q = \frac{dQ}{d\mu}$ exists, then by the chain rule, we recover a more familiar form $$ K(Q \mid\mid P) = \int_X \log\brac{\frac{\frac{dQ}{d\mu}}{\frac{dP}{d\mu}}} \frac{dQ}{d\mu} d\mu = \int_X q \log \frac{q}{p} d\mu. $$ In the definition we gave above, we have been using the standard Lebesgue measure on $\R^n$. As another example, if we have the counting measure $\mu_c$ on a discrete measure space $X$ and probability measures $P$ and $Q$ on $X$, then the expression becomes, $$ K(Q \mid\mid P) = \sum_{x \in X}Q(x) \log \frac{Q(x)}{P(x)}. $$</p></details><p>I find it helpful to understand a little about <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shanon’s information entropy</a><span sidenote="">and we shall need the concept of information entropy in our study anyway.</span> in order to get an intutitive sense of KL-divergence. Informally, Shanon’s information quantifies how surprised one should be when an event that was thought to have probability $p \in [0, 1]$ occurs. Intuitively, the observation “the sun rose in the east this morning” has less information content than “it was raining this morning in Perth” or “the Dow Jones index rose by 3% this morning”. Think about which “insider knowledge” - knowledge that dispels prior uncertainty by increasing the event probability to 1 - will you pay more to obtain to give an edge in a bet. Stated another way, the information content quantifies ignorance about an event prior to observation.</p><p>A little more formally, Shanon information is another way of encoding probability with the following properties</p><ul><li>Information content is a monotonically decreasing non-negative function $I: [0, 1] \to \R_{\geq 0}$ of the probability $p$ of the event.<li>Events with complete certainty have zero information.<li>Information of independent events adds. These properties together with some smoothness assumptions on $I$ forces</ul>\[I(p) = -\log p.\]<p>If we have a set of events following a probability distribution<span sidenote="">more precisely, a sigma algebra of events with a probability measure, i.e. a probability space</span>, we can quantify the <em>expected</em> amount of information, or the expected amount of surprise if we were to observe a large amount events drawn from the distribution. See Shanon’s original 1948 paper (<a href="#ref-3">Shanon, 1948</a>) that started off information theory for formal derivation and discussion.</p><div class="def"><p> The <span def="">Shanon entropy</span> of a random variable $X$ taking on finite number of values ${x_1, \dots, x_n}$ each with probability $p_i = p(X = x_i)$ is given by the following expectation expression<span sidenote="">we use the letter $S$ for entropy here, consistent with statistical physics. However, Shanon originally uses the letter $H$ for this quantity, named after Boltzmann's <a href="https://en.wikipedia.org/wiki/H-theorem">H-theorem</a> for thermodynamics.</span> $$ S(p) = \E_p[-\log p(X)] = -\sum_{x \in X} p(x) \log p(x) $$</p><p> For continuous distribution $p(x)$, we replace the sum with integral $$ S(p) = \int p(x) \log p(x) dx. $$</p></div><p>Observe that even though low probability events has high information content $-\log p(x)$, they occur with less frequency $p(x)$. This push and pull, together with the fact that probability sum to unity, means that the distribution with the highest entropy is the uniform distribution<span sidenote="">if there is no other constraints and if a uniform distribution can be defined on the space. </span></p><h2 id="theory-of-statistical-learning">Theory of Statistical Learning</h2><p>\(-\frac{1}{N} \log L_N(w) \to K(w) - S(q)\)</p><p>But minimisation of $-\log L_N(w)$ is not equivalent to minimisation of $K(w)$ (which would give the closest estimate to $q(x)$). So MLE is not the only valid or the best statistical estimation method.</p><h2 id="fisher-information-matrix">Fisher Information Matrix</h2><h1 id="singular-models">Singular Models</h1><p>Various equivalent definitions of singular models (via FIM and K(w)). Some implications.</p><h1 id="model-selection">Model Selection</h1><h1 id="misc">MISC</h1><p>Seminar blackboard draft</p><iframe src="https://drive.google.com/file/d/1To8ZrCwueWvBIL1rwQXPquBosMhhVe92/preview"></iframe><h1 id="references">References</h1><p id="ref-1"> Sumio Watanabe. 2009. "Algebraic Geometry and Statistical Learning Theory." Cambridge University Press.</p><p id="ref-2"> Sumio Watanabe. 2018, "Mathematical Theory of Bayesian Statistics." CRC Press, Chapman and Hall/CRC.</p><p id="ref-3"> Shannon, C. E. 1948. “A Mathematical Theory of Communication.” The Bell System Technical Journal 27 (3): 379–423.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/singular-learning-theory/'>singular-learning-theory</a>, <a href='/categories/singular-learning-theory-lecture-series/'>singular-learning-theory-lecture-series</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/singular-learning-theory/" class="post-tag no-text-decoration" >singular-learning-theory</a> <a href="/tags/algebraic-geometry/" class="post-tag no-text-decoration" >algebraic-geometry</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >machine-learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Singular Learning Theory - Part 1 - Probably Singular&url=https://edmundlth.github.io/posts/singular-learning-theory-part-1/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Singular Learning Theory - Part 1 - Probably Singular&u=https://edmundlth.github.io/posts/singular-learning-theory-part-1/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/statistical-learning-theory-via-a-coin-flipping-demon/">Statistical Learning Theory Via A Coin Flipping Demon</a><li><a href="/posts/singular-learning-theory-part-1/">Singular Learning Theory - Part 1</a><li><a href="/posts/hartshorne-algebraic-geometry-1.1/">Algebraic Geometry - Hartshorne 1.1</a><li><a href="/posts/introduction-to-grobner-basis/">Introduction to Grobner Basis</a><li><a href="/posts/overview-of-singular-learning-theory/">Overview of Singular Learning Theory</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/machine-learning/">machine-learning</a> <a class="post-tag" href="/tags/singular-learning-theory/">singular-learning-theory</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/seminar-notes/">seminar-notes</a> <a class="post-tag" href="/tags/algebraic-geometry/">algebraic-geometry</a> <a class="post-tag" href="/tags/grobner-basis/">grobner-basis</a> <a class="post-tag" href="/tags/hartshorne/">hartshorne</a> <a class="post-tag" href="/tags/real-log-canonical-threshold/">real-log-canonical-threshold</a></div></div></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/introduction-to-singular-models/"><div class="card-body"> <span class="timeago small" >Jun 1, 2021<i class="unloaded">2021-06-01T00:00:00+10:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Introduction to Singular Learning Theory</h3><div class="text-muted small"><p> Introduction Let’s first set the context. Imagine we are given a data generating process $q(x)$ where we can ask for $N \in \N$ samples Throughout, we assume that the process generates i.i.d. sampl...</p></div></div></a></div><div class="card"> <a href="/posts/statistical-learning-theory-via-a-coin-flipping-demon/"><div class="card-body"> <span class="timeago small" >Jan 13<i class="unloaded">2022-01-13T00:00:00+11:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Statistical Learning Theory Via A Coin Flipping Demon</h3><div class="text-muted small"><p> “It is simplicity itself, so absurdly simple that an explanation is superfluous; and yet it may serve to define the limits of observation and of deduction." Sherlock Holmes, "The Sign...</p></div></div></a></div><div class="card"> <a href="/posts/ladder-to-free-energy-asymptotics/"><div class="card-body"> <span class="timeago small" >Feb 24<i class="unloaded">2022-02-24T00:00:00+11:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Ladder to Free Energy Asymptotics</h3><div class="text-muted small"><p></p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/overview-of-singular-learning-theory/" class="btn btn-outline-primary" prompt="Older"><p>Overview of Singular Learning Theory</p></a> <a href="/posts/hartshorne-algebraic-geometry-1.1/" class="btn btn-outline-primary" prompt="Newer"><p>Algebraic Geometry - Hartshorne 1.1</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://edmundlth.github.io/posts/singular-learning-theory-part-1/'; this.page.identifier = '/posts/singular-learning-theory-part-1/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://probably-singular.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (modeToggle !== null) { modeToggle.addEventListener('click', reloadDisqus); window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/edmundlth">Edmund Lau</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/singular-learning-theory/">singular learning theory</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/seminar-notes/">seminar notes</a> <a class="post-tag" href="/tags/algebraic-geometry/">algebraic geometry</a> <a class="post-tag" href="/tags/grobner-basis/">grobner basis</a> <a class="post-tag" href="/tags/hartshorne/">hartshorne</a> <a class="post-tag" href="/tags/real-log-canonical-threshold/">real log canonical threshold</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
