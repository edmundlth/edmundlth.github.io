<!DOCTYPE html><html lang="en" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Statistical Learning Theory Via A Coin Flipping Demon" /><meta property="og:locale" content="en" /><meta name="description" content="Singular Learning Theory" /><meta property="og:description" content="Singular Learning Theory" /><link rel="canonical" href="https://edmundlth.github.io/posts/statistical-learning-theory-via-a-coin-flipping-demon/" /><meta property="og:url" content="https://edmundlth.github.io/posts/statistical-learning-theory-via-a-coin-flipping-demon/" /><meta property="og:site_name" content="Probably Singular" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-13T00:00:00+11:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Statistical Learning Theory Via A Coin Flipping Demon" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-02-09T15:16:47+11:00","datePublished":"2022-01-13T00:00:00+11:00","description":"Singular Learning Theory","headline":"Statistical Learning Theory Via A Coin Flipping Demon","mainEntityOfPage":{"@type":"WebPage","@id":"https://edmundlth.github.io/posts/statistical-learning-theory-via-a-coin-flipping-demon/"},"url":"https://edmundlth.github.io/posts/statistical-learning-theory-via-a-coin-flipping-demon/"}</script><title>Statistical Learning Theory Via A Coin Flipping Demon | Probably Singular</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Probably Singular"><meta name="application-name" content="Probably Singular"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://d3awt3adt4koyi.cloudfront.net/images/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Probably Singular</a></div><div class="site-subtitle font-italic">Mathematics and Machine Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/edmundlth" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['edmundlth95','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Statistical Learning Theory Via A Coin Flipping Demon</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <script src="/assets/js/sidenote.js"></script><div id="latex-macros" style="display:none"> $$ \newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\P}{\mathbb{P}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\V}{\mathbb{V}} \newcommand{\E}{\mathbb{E}} \newcommand{\A}{\mathbb{A}} \newcommand{\F}{\mathbb{F}} \newcommand{\L}{\mathcal{L}} \newcommand{\sgn}{\mathrm{sgn}} \newcommand{\brac}[1]{\left ( #1 \right)} \newcommand{\set}[1]{\left \{ #1 \right \}} \newcommand{\st}{\, \mid \,} \newcommand{\wh}{\,|\,} \newcommand{\abrac}[1]{\left \langle #1 \right \rangle} \newcommand{\sqbrac}[1]{\left [ #1 \right]} $$</div><h1 data-toc-skip>Statistical Learning Theory Via A Coin Flipping Demon</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Edmund Lau </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Jan 13, 2022, 12:00 AM +1100" >Jan 13<i class="unloaded">2022-01-13T00:00:00+11:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Wed, Feb 9, 2022, 12:16 PM +0800" >Feb 9<i class="unloaded">2022-02-09T15:16:47+11:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3444 words">19 min read</span></div></div><div class="post-content"> <img data-proofer-ignore data-src="https://d3awt3adt4koyi.cloudfront.net/images/2022-01-13/coin-flipping-demon-model1.jpg" class="preview-img" alt="image not found" width="100%" ><div class="epi"><blockquote> “It is simplicity itself, so absurdly simple that an explanation is superfluous; and yet it may serve to define the limits of observation and of deduction." <cite>Sherlock Holmes, "The Sign of Four"</cite></blockquote></div><p>In this post, a simple statistical inference task of a modeling a coin flip shall be our vehicle to introduce important statistical concepts. These concepts will become central to our study in Singular Learning theory. We shall show our calculations with excruciating details in hope that such simple case will be instructive and serve to define the limits of our tools in extracting exact results.</p><details> <summary> Post summary: </summary><p> To model the outcome of a coin flip, we introduce a general way to do statistical inference by constructing a <b>parameterised model</b> - a family of probability distributions, each a possible approximation to the process that generates the coin flips. We introduce the <b> likelihood function </b>. It gives the probability of observing the given data set assuming they are generated by the model at a specified parameter.</p><p> We introduce a popular statistical estimation method, the <b>Maximum Likelihood</b> method, to estimate the outcome of a coin flip. With this method, the likelihood function is treated as an objective function to be maximised and the model at the maximum is used to approximate the truth. We point out that <b>the result of such an inference procedure is itself random</b>. Specifically, it depends on the random sampling that generates the data set. It is the role of a <b>statistical learning theory</b> to clarify if a certain statistical estimation procedure is to be employed<ul><li>What is the expected performance given subject to the randomness of training samples?<li>For what class of true data generating process would the procedure approximate well?</ul></p><p> Using the normal approximation to the binomial distribution, we shall see that the maximum likelihood estimator for the coin flip model is <b>asymptotically normal</b> and <b>asymptotically efficient</b> two desirable properties that is the hallmark of <b>regular model</b> but are absent in <b>singular models</b>.</p></details><ul><li><a href="#the-situation-the-dataset">The Situation, The Dataset</a><li><a href="#statistical-inference">Statistical Inference</a><ul><li><a href="#models">Models</a><li><a href="#learning-and-inference">Learning and Inference</a></ul><li><a href="#the-adventure-of-maximum-likelihood-estimation">The Adventure of Maximum Likelihood Estimation</a><ul><li><a href="#empirical-log-loss-and-log-likelihood-ratio">Empirical Log Loss and Log Likelihood Ratio</a><li><a href="#finally-some-calculations">Finally Some Calculations</a><li><a href="#so-how-did-we-do">So how did we do?</a></ul><li><a href="#a-broader-theory-of-mle-and-fisher-information">A Broader Theory of MLE and Fisher Information</a><li><a href="#role-of-statistical-learning-theory">Role of Statistical Learning Theory</a></ul><h1 id="the-situation-the-dataset">The Situation, The Dataset</h1><p>Suppose there is a coin-flipping demon on the other side of a door. He offers us a deal. We can bet a number of years of our remaining life. If his next coin flip turns out to be head, he will pay us double what we bet. Otherwise, he keeps the bet. Rather uncharacteristically<span sidenote="">or maybe it’s just a deception</span>, he offers to first perform $N$ flips and let us look at the results. We shall call this set of observations</p>\[D_N = \set{X_1, X_2, \dots, X_N}, \quad X_i \in \set{H, T}\]<p>our <span def=""> dataset </span> or <span def=""> training samples </span>.</p><p>As a pure utilitarian, we decide to not run. Which means we better have a really good estimate of the probability $\hat{p}$ of the next flip turning out heads <span sidenote=""> since most sound betting strategy relies on it. For instance, the <a href="https://en.wikipedia.org/wiki/Kelly_criterion">Kelly’s criterion </a> gives the optimal fraction of our life in this situation as $2 \hat{p} - 1$ </span>.</p><h1 id="statistical-inference">Statistical Inference</h1><h2 id="models">Models</h2><p>So we are in a situation where we are given a <em>training set</em>, $D_N$, of previous coin flips and our task is to estimate the probability $\hat{p}$ that the next flip comes up head.</p><p>Let $h$ denote the number of heads in the dataset $D_N$ and $t = N - h$ the number of tails.</p><p class="block"> <b>Model 0. </b> We can simply <i>guess</i> that it's a fair coin so that $\hat{p} = 1/2$. This is a not a good inference method<span sidenote=""> not less because a demon wouldn't be <i>fair</i></span>. There is no <i>learning</i> involved at all. We ignored the dataset, which should be a valuable resource in dealing with a demon. Even in this case, we are still left with questions like <i>when</i> exactly is this method bad<span sidenote="">just so that no one will come and say "I told you so" when the demon actually uses a fair coin</span> and how to quantify exactly how bad the method is.</p><p class="block"> <b>Model 1. </b> We can instead construct a family of hypothesis, parameterised by $w \in [0, 1]$. Each $w$ represent a guess that the probability of landing heads $=w$. We say that we are defining a <span def="">model</span> $$ p(X \mid w) = \begin{cases} w, &amp; X = H \\ 1 - w, &amp; X = T \end{cases} $$ with <span def="">parameter</span> $w$ taking value in the <span def="">parameter space</span> $W = [0, 1]$. We can now reason as follow: <br /> <i> If the probability of landing heads were to be $w$, then the probability that the dataset $D_N$ that we observed get generated is $$ p(D_N \mid w) = \prod_{i = 1}^N p(X_i \mid w) = w^h (1 - w)^t. $$ </i><br /><br /> Notice that we are <i>assuming </i> that, on the other side of the door, the demon is consistently throwing the same coin in the same way, so that we get a constant probability of landing heads. We call this true probability of landing heads the <span def="">true parameter $w_0$</span>. This means that each $X_i$ follows an <span def="">identical</span> distribution, $p(H) = w_0$, and that each observation is <span def="">independent</span> of all others. Together, this is the common <span def="">independent and identically distributed (i.i.d)</span> assumption that allows us to factorise $p(D_N\mid w)$ as we did above<span sidenote="">We could stipulate additionally that the order of $X_i$ doesn't matter, introducing a factor of $\binom{N}{h}$ making the above the <a href="https://en.wikipedia.org/wiki/Binomial_distribution#Definitions">binomial distribution</a>. But we omit this for exposition uniformity. The binomial coefficient will be reintroduced whenever we sum over these sequences anyway. </span>. We call the probability of observing a given dataset for a given parameter the <span def="">likelihood function</span> $$l_N(w) = p(D_N \mid w)$$ which, to emphasise, is a function of paramter $w$ with the dataset $D_N$ fixed.</p><p class="block"> <b>Model 2. </b> <img data-proofer-ignore data-src="https://d3awt3adt4koyi.cloudfront.net/images/2022-01-13/coin-flipping-demon-model2.jpg" alt="Randomly choosing being a fair and bias coin before every coin toss." /> But the demon could be more ... demonic. Every time he does a flip, he could've been choosing one of two coins at random from a box, only one of which is a fair coin. We can introduce another parameter $s \in [0,1]$ for representing the probability that the fair coin is chosen and $w \in [0, 1]$ represent the bias - probability of landing heads - for the other coin. We have now a two dimensional<span sidenote="">but still compact</span> paramter space $(s, w) \in [0, 1]^2$. The hypothesis at each parameter is given by $$ \begin{align*} p(H \mid s, w) &amp;= \frac{1}{2}s + w (1 -s) \\ p(T \mid s, w) &amp;= \frac{1}{2}s + (1 - w) (1 - s) \end{align*} $$ and the likelihood function becomes $$ l_N(w) = \prod_{i = 1}^Np(X_i \mid s, w) = \brac{\frac{1}{2}s + w (1 -s)}^h \brac{\frac{1}{2}s + (1 - w)(1 -s)}^t $$ which again uses the i.i.d assumption.</p><h2 id="learning-and-inference">Learning and Inference</h2><p>We could continue to imagine more and more demonic opponent on the other side, but we should first see what can the models do for us.</p><p>We can continue the reasoning with likelihood function as follow:</p><p><i> We now have $l_N$ giving us the probability of observing $D_N$ for specific parameter.<br /> Surely the most likely $w$ is the one that give the highest probability for the observed data?<span sidenote=""> this reminds me a little of the <a href="https://en.wikipedia.org/wiki/Anthropic_principle">antropic principle.</a></span> Hence we should set the estimate $\hat{p}$ to be the parameter $w$ that maximises $l_N(w)$. </i></p><p>This gives us the statistical inference method known as <span def="">Maximum Likelihood Estimate (MLE)</span>, i.e. find the single parameter <span sidenote="">this makes it a <i>point estimate.</i></span> $w$ that maximises the likelihood function.</p><p>This is a natural line of reasoning<span sidenote="">there was a time when I was only taught this method of statistical estimation and think this is the only way.</span>, and indeed it is in some sense the correct method for this situation.</p><p>However, MLE assumed quite a few things.</p><ul><li>MLE says “surely another coin with a different probability of landing heads would produce quite different looking observations $D_N$?”. Well, not really. What if there is only one observation $N = 1$? Let’s say $D_1 = \set{H}$. Surely a whole range of $w$ could’ve plausibly generated it<span sidenote="">as we shall see, the MLE answer in this case would be at the boundary of $[0, 1]$, $\hat{p} = 1$ which, at least to me, doesn’t seem to be a well-justified answer.</span>?<li>“Ah, but if $N$ is truly huge, small difference in $w$ will result in very different $D_N$”, says MLE. We shall see that this is indeed true. As the number of observations grow, it becomes <strong>asymptotically</strong> impossible for another parameter but the true one to generate the sequence of coin flips observed.<li>BUT, that is only true if we made the assumption that the true parameter is contained in the model! This is the <span def="">realisability</span> assumption. For instance, if the demon is really following the coin flipping recipe laid out in model 2 above, then maximising the likelihood function of model 1<span sidenote=""> how much worse is model 0?</span>wouldn’t give us accurate answer, not even asymptotically.<li>MLE doesn’t provide the answer to the quenstion “what is the most likely parameter given the observation” $= p(w \mid D_N)$, which is a distinct question to “what is the most likely observation given the parameter” $= p(D_N \mid w)$.</ul><h1 id="the-adventure-of-maximum-likelihood-estimation">The Adventure of Maximum Likelihood Estimation</h1><p>Despite the above misgiving, we shall push through with MLE of model 1. We shall have explicit computation and introduce several important statistical concepts along the way. We shall see in what sense is MLE a good estimator in this case and what could go wrong in other situations where we might like to use the same procedure.</p><p>We shall also make the realisability assumption where the demon’s coin flip follows the following true distribution</p>\[\begin{align*} q(H) &amp;= w_0 \\ q(T) &amp;= (1 - w_0) \end{align*}\]<p>for some true paramter $w_0 \in [0, 1]$. Meaning $q(x) = p(x \mid w_0)$.</p><h2 id="empirical-log-loss-and-log-likelihood-ratio">Empirical Log Loss and Log Likelihood Ratio</h2><p>We wish to set $\hat{p} =$ the maximum of the likelihood function</p>\[l_N(w) = w^h (1 - w)^t.\]<p>If $N$ is a large number, the above expression produces really small numbers which can be numerically challenging to handle. We usually do a monotonic<span sidenote=""> hence doesn’t change any extrema of the function </span>transform by taking its logarithm, which also happily simplifies several calculations. In fact, we shall introduce the <span def="">empirical log loss function</span></p>\[L_N(w) = -\frac{1}{N} \log p(D_N \mid w) = -\frac{1}{N} \sum_{i = 1}^N \log p(X_i \mid w)\]<p>which we shall aim to <em>minimise</em><span sidenote="">hence the usage of the term “loss”.</span>. In our case, we get</p>\[L_N(w) = \frac{1}{N} \brac{\sum_{X_i = H} \log w + \sum_{X_i = T} \log (1 - w)} = \frac{1}{N} \sqbrac{h \log w + t \log (1 - w)}\]<p>Below is a little tangent justifying the introduction of this quantity and its connection to the log likelihood ratio function which will be an important information theoretic quantity for our study.</p><details open=""> <summary>Connection to log likelihood ratio</summary><p> The <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)#Definition">Shanon entropy </a>of the true distribution is given by $$ S(q) = -\E_q(\log q(X)) = -q(H)\log q(H) + -q(T) \log q(T) $$ which can be approximated by the <span def="">empirical entropy</span> using the observation from the data set $D_N$ $$ S_N(q) = -\frac{1}{N} \sum_{i = 1}^N \log q(X_i). $$ Adding this quantity to the empirical log loss function we get the <span def="">empirical log likelihood ratio </span> $$ K_N(w) = \frac{1}{N} \sum_{i = 1}^N \log \frac{q(X_i)}{p(X_i \mid w)} = -\frac{1}{N} \sum_{i = 1}^N \log p(X_i \mid w) + \frac{1}{N} \sum_{i = 1}^N \log q(X_i) = L_N(w) - S_N(q) $$ which is the empirical estimate to the central object of study, the <span def=""> expected log likelihood rato</span> better known as the <span def=""> <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a></span> of the model at $w$ to the true distribution $$ K(w) = D_{KL}(q(x) \mid \mid p(x \mid w)) = \E_X\sqbrac{\log \frac{q(X)}{p(X \mid w)}}. $$ We note that minimising $K_N(w)$ is equivalent to minimising $L_N(w)$ since they differ by an additive constant that is independent of $w$.</p></details><h2 id="finally-some-calculations">Finally Some Calculations</h2><p>Since our objective function $L_N(w)$ is a differentiable function, we shall use the stationary point criterion for finding local minima.</p><iframe src="https://drive.google.com/file/d/1L8MurEGH8YJzsSwzsUybbcxhfJOcUHvb/preview" title="Minimising average log loss."></iframe><p>So, we find that maximimum likelihood estimate is given by</p>\[\hat{p} = \hat{w} = \frac{h}{N}.\]<p>It’s important to emphasise again that $h$ in the expression above is the number of heads in the given data set $D_N$, <em>it depends on the given set of observations so is itself a random variable!</em><span sidenote="">indeed, even functions we defined like $l_N(w)$, $L_N(w)$, $K_N(w)$ are all random variables, they just take values in the space of functions. This is why proper investigation into their behaviour in statistical learning theory requires functional analysis. </span></p><h2 id="so-how-did-we-do">So how did we do?</h2><p>Though we now have our estimate, there is still a concern that we might have gotten an atypical data set which makes our guess wildly inaccurate. We need some way to quantify the uncertainty of our $\hat{w} = h / N$ estimate. Fortunately, we can do some exact calculation in this case.</p><iframe src="https://drive.google.com/file/d/1BiPdUnsiFyvqI-nsOpYpGEtwH0og-gv0/preview"></iframe><p>The asymptotic normality $\hat{w} \to N(Nw_0, Nw_0 (1 - w_0))$ uses the <a href="https://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation">normal approximation to binomial distribution</a>.<span sidenote=""> can be proven by apply the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a> to sums of Bernoulli random variables or directly use a classic result of <a href="https://en.wikipedia.org/wiki/De_Moivre%E2%80%93Laplace_theorem">de Moivre-Laplace</a>.</span></p><p>The array of plots below shows the normal density superimposed on histograms of $\hat{w}$ for different values of $N$ and $w_0$. Observe that the normal approximation works better for large values of $N$ and away from pathological values of $w_0 = 0, 1$. The closer the true parameter $w_0$ is to the boundary, the large the data set needs to be for the normal distribution to correctly approximate the density away from the mean<span sidenote="">Imagine $w_0$ being close to the boundary, then when $N$ is small the variance $w_0(1 - w_0) / N$ is large, which makes a large portion of the normal density extends in to the “imppossible” region $ \hat{w} \not\in [0, 1]$. </span>.</p><p><img data-proofer-ignore data-src="https://d3awt3adt4koyi.cloudfront.net/images/2022-01-13/normal_approx_to_binom.png" alt="Normal approximation to binomial distribution" /></p><h1 id="a-broader-theory-of-mle-and-fisher-information">A Broader Theory of MLE and Fisher Information</h1><p>Having been forced by the demon to go through these exercises in statistical inference we noticed a few things about MLE for model 1:</p><ul><li>The maximum likelihood parameter exist and is unique, even when it turns out to be at the boundary $\hat{w} = 0, 1$.<li>It is an <em>unbiased estimator</em>, meaning the expected MLE prediction gives the true paramter (again, assuming <em>realisability</em>.)<li>It is asymptotically normal, meaning as the number of observations $N \to \infty$, the <em>distribution</em> of $\hat{w}$ approaches that of the normal distribution.<li>It turns out that the form the of asymptotic normal distribution implies that $\hat{w}$ achieves <span def=""> asymptotic efficiency </span>, meaning it achieves the lowest variance possible for any unbiased estimator as prescribed by the <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">Cramer-Rao inequality</a>.</ul><p>Before we state the Cramer-Rao bound, we need a few preliminaries. It turns out that the above is actually true for MLE of a large class of models $p(x \mid w)$ modeling some true distribution $q(x)$ satisfying some regularity conditions<span sidenote=""> which turns out to be far too strong for general statistical learning theory. Hence the need to study “Singular Models”. </span>.</p><p>Even with slightly more complicated models, it is unlikely that we will have the luxury of computing the estimator distribution exactly as we did in this simple case. However, taking limits ($N \to \infty$) is always a good way to wash away the details.</p><p>Perhaps the most famous among the tools for washing away details is the Central Limit Theorem</p><div class="theorem"> (Central Limit Theorem) Given a sequence of i.i.d. random variables $\set{X_1, X_2, \dots}$, each with finite means and variance, $\mu = \E[X_i] &lt; \infty$ and $0 &lt; \sigma^2 = \V[X_i] &lt; \infty$, we can take partial averages $S_n = \frac{1}{n} \sum_{i = 1}^n X_i$. Then, as $n \to \infty$, $S_n$ converges in distribution to $N(\mu, \sigma^2/ \sqrt{n})$, $$ \sqrt{n}(S_n - \mu) \overset{d}{\to} N(0, \sigma^2). $$ meaning the probability density of $\sqrt{n}(S_n - \mu)$ converges pointwise <span sidenote="">at points of continuity. see <a href="https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_distribution">here</a>.</span> to that of the normal density $$ \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{\sigma^2}}. $$</div><p>So, if somehow our estimators are sums of increasingly numerous i.i.d. random variables, we have asymptotic normality. Just like in the case of the coin flip where the number of heads $h$ is given by sums of i.i.d. random variables $\sim Bernoulli(w_0)$. We might not have that in general. However, it turns out that the spread of MLE estimator is related to the following object.</p><div class="def"> For a given statistical model $p(x \mid w)$ with $x \in \R^n$ and $w \in \R^d$, the <span def=""> Fisher information matrix</span> is given by $$ I(w) = \mathrm{CovariantMatrix}(\frac{\partial}{\partial w_j}\log p(x \mid w)) $$ where the covariance is taken with $p(x \mid w)$ itself as probability distribution. Explicitly in component form, $$ I_{jk}(w) = \int \brac{\frac{\partial}{\partial w_j}\log p(x \mid w)}\brac{\frac{\partial}{\partial w_k}\log p(x \mid w)} p(x \mid w) dx. $$</div><p>In the 1D case, this reduces to a scalar function</p>\[I(w) = \V_{X \sim p(x \mid w)}(\frac{\partial}{\partial w}\log p(X \mid w)) = \int \brac{\frac{\partial}{\partial w}\log p(x \mid w)}^2 p(x \mid w) dx.\]<p>We interpret this as answering the following question:</p><p class="block"> <i>If the data were sampled from the distribution at $w$, $X_i \sim p(x \mid w)$, what is the variance of the $\frac{d}{dw} \log p(x \mid w)$, the log-likelihood ratio. </i></p><p>Recall that we use the condition that $\frac{d}{dw} \log p(x \mid w)$ to find (local) maximum of the likelihood function which we then take as our estimate of the true parameter. So, informally, $I(w)$ is related to the spread of our MLE prediction.</p><p>Computing this quantity for our coin toss model,</p><iframe src="https://drive.google.com/file/d/16VY8kw-E38-mKctYcx15LoGxMPLYNn6v/preview"></iframe><p>we get</p>\[I(w) = \frac{1}{w(1 - w)}.\]<p>The lemma proven above shows that $I(w) = \E_{X \sim p(x \mid w)} \sqbrac{-\partial^2_w \log p(x \mid w)}$. This gives another intuition of what $I(w)$ measures:</p><p class="block"> <i>If the data were sampled from the distribution at $w$, what is the expected curvature of the log-likelihood near $w$?</i> <br /> <i>Said another way: If the true parameter $w_0$ were to be at $w$, what is the expected curvature of the log-likelihood near the true parameter? </i></p><p>From the calculation, at the true parameter $I(w_0) = (n \sigma^2)^{-1}$ where $\sigma^2 = w_0(1 - w_0) / N$ is the variance of $\hat{w}$ computed before. This relationship between estimator variance and Fisher information is not a coincidence. It is evidence that MLE for our model above is <span def="">asymptotically efficient</span>. This means that the $\sigma^2$ is the tightest asymptotic spread of our estimator possible. Put another way, our MLE achieves the Cramer-Rao lower bound stated below.</p><div class="theorem"> (Cramer-Rao bound) Let $\hat{w}$ is an unbiased estimator for a model $p(x \mid w)$ computed from a set of i.i.d. samples $\set{X_1, \dots X_N}$, $X_i \sim p(x \mid w)$. Let$V_{ij} = \E\sqbrac{(\hat{w}_i - w_i)(\hat{w}_j - w_j)}$ be the covariance matrix and $I_{ij}(w)$ the Fisher information matrix for the model. Then $$ V \geq \brac{N I(w)}^{-1} $$ where inequality of matrices is defined as $A \leq B \iff A - B$ is positive semidefinite.</div><p>The theorem says that the Fisher information matrix give the highest precision or the lowest possible variance <span sidenote=""> precision $= 1 / $ variance</span> <em>any</em> unbiased estimator can achieve. In fact, this is more general than just the MLE for our simple coin toss case.</p><div class="theorem"> (MLE's are asymptotically efficient) Let $p(x \mid w)$, $w \in W \subset \R^d$ be a parametric model. Let $D_N = \set{X_1, \dots, X_N}$ be a dataset of size $N$ with i.i.d. samples from the probability density at a true parameter $w_0$, $X_i \sim p(x \mid w_0)$ with $w_0$ being in the interior of $W$. Under some regularity conditions, the maximum likelihood estimator is asymptotically normal and efficient: $$ \sqrt{N} \brac{\hat{w}_{MLE} - w_0} \overset{d}{\to} N(0, I(w_0)^-1) $$ where $I(w)$ is the Fisher information matrix for the model.</div><details> <summary> Some regularity conditions: </summary> It is rather complex to carefully state the regularity conditions here. For rigorous discussions, see (<a href="#ref-1">Lehmann, 2005</a>, chapter 12). But the conditions frequently involves<ul><li> $W$ is an open subset.<li> The model is identifiable. Meaning distinct parameters gives distinct probability density.<li> The density $p(x \mid w)$ for any $w$ has common support.<li> $p(x \mid w)$ is thrice continuously differentiable with respect to $w$.<li> The model has positive definite Fisher information matrix.</ul></details><p>Also, notice that the lower bound does not apply when $I(w) = 0$ such as when $w = 0, 1$ in our coin toss model and more generally when $I_{ij}(w)$ is not an invertible matrix.</p><h1 id="role-of-statistical-learning-theory">Role of Statistical Learning Theory</h1><p>The role of statistical learning theory is to clarify the performance of procedures like MLE we used above. What if it is not realisable? This is a rather common case for modeling complex processes. What if the true parameter is at a singularity of the model? Nearby points are affected as well. What happen when $N$ is small?</p><p id="ref-1"> Lehmann, E. L., and Joseph P. Romano. 2005. Testing Statistical Hypotheses. Springer, New York, NY.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/singular-learning-theory/'>singular-learning-theory</a>, <a href='/categories/singular-learning-theory-lecture-series/'>singular-learning-theory-lecture-series</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/singular-learning-theory/" class="post-tag no-text-decoration" >singular-learning-theory</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >machine-learning</a> <a href="/tags/statistics/" class="post-tag no-text-decoration" >statistics</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Statistical Learning Theory Via A Coin Flipping Demon - Probably Singular&url=https://edmundlth.github.io/posts/statistical-learning-theory-via-a-coin-flipping-demon/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Statistical Learning Theory Via A Coin Flipping Demon - Probably Singular&u=https://edmundlth.github.io/posts/statistical-learning-theory-via-a-coin-flipping-demon/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/statistical-learning-theory-via-a-coin-flipping-demon/">Statistical Learning Theory Via A Coin Flipping Demon</a><li><a href="/posts/singular-learning-theory-part-1/">Singular Learning Theory - Part 1</a><li><a href="/posts/hartshorne-algebraic-geometry-1.1/">Algebraic Geometry - Hartshorne 1.1</a><li><a href="/posts/introduction-to-grobner-basis/">Introduction to Grobner Basis</a><li><a href="/posts/overview-of-singular-learning-theory/">Overview of Singular Learning Theory</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/machine-learning/">machine-learning</a> <a class="post-tag" href="/tags/singular-learning-theory/">singular-learning-theory</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/seminar-notes/">seminar-notes</a> <a class="post-tag" href="/tags/algebraic-geometry/">algebraic-geometry</a> <a class="post-tag" href="/tags/grobner-basis/">grobner-basis</a> <a class="post-tag" href="/tags/hartshorne/">hartshorne</a> <a class="post-tag" href="/tags/real-log-canonical-threshold/">real-log-canonical-threshold</a></div></div></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/ladder-to-free-energy-asymptotics/"><div class="card-body"> <span class="timeago small" >Feb 24<i class="unloaded">2022-02-24T00:00:00+11:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Ladder to Free Energy Asymptotics</h3><div class="text-muted small"><p></p></div></div></a></div><div class="card"> <a href="/posts/climbing-ladder-to-free-energy-asymptotics/"><div class="card-body"> <span class="timeago small" >Mar 17<i class="unloaded">2022-03-17T00:00:00+11:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Climbing Ladder to Free Energy Asymptotics</h3><div class="text-muted small"><p></p></div></div></a></div><div class="card"> <a href="/posts/introducing-standard-form-of-log-loss/"><div class="card-body"> <span class="timeago small" >Mar 24<i class="unloaded">2022-03-24T00:00:00+11:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Introducing Standard Form of Log-loss</h3><div class="text-muted small"><p></p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/hartshorne-algebraic-geometry-1.1/" class="btn btn-outline-primary" prompt="Older"><p>Algebraic Geometry - Hartshorne 1.1</p></a> <a href="/posts/ladder-to-free-energy-asymptotics/" class="btn btn-outline-primary" prompt="Newer"><p>Ladder to Free Energy Asymptotics</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://edmundlth.github.io/posts/statistical-learning-theory-via-a-coin-flipping-demon/'; this.page.identifier = '/posts/statistical-learning-theory-via-a-coin-flipping-demon/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://probably-singular.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (modeToggle !== null) { modeToggle.addEventListener('click', reloadDisqus); window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/edmundlth">Edmund Lau</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/singular-learning-theory/">singular learning theory</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/seminar-notes/">seminar notes</a> <a class="post-tag" href="/tags/algebraic-geometry/">algebraic geometry</a> <a class="post-tag" href="/tags/grobner-basis/">grobner basis</a> <a class="post-tag" href="/tags/hartshorne/">hartshorne</a> <a class="post-tag" href="/tags/real-log-canonical-threshold/">real log canonical threshold</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
