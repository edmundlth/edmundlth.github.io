<!DOCTYPE html><html lang="en" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Introduction to Singular Learning Theory" /><meta property="og:locale" content="en" /><meta name="description" content="Introduction Let’s first set the context. Imagine we are given a data generating process $q(x)$ where we can ask for $N \in \N$ samples Throughout, we assume that the process generates i.i.d. samples. In particular, \(X_i \sim q\) for all \(i\), with \(q\) unchanging as we ask for more samples. However, we note that this is a simplification: one would imagine that an (artificial) intelligent &quot;student&quot; would judiciously ask for more informative examples from a &quot;teacher&quot; process. , $D_N = { X_1, \dots, X_N }$. Our goal is to learn a distribution $p(x)$ from which we can make inferences about the data generating process itself. In other words, the task of a statistical learning machine is to discover structures and properties $q(x)$ from training examples $D_N$. Some examples: Deterministic data: If $q$ generates the result of “$1 + 1$”, we can set $p(x_1) = 1$ where $x_1 = 2$ is the first “measurement” or request we made to the data generating process. Here the learning process recover everything we wish to know about $q$ just from the first data point, i.e. $p = q$. As such, there is no reason to deviate from this learning process." /><meta property="og:description" content="Introduction Let’s first set the context. Imagine we are given a data generating process $q(x)$ where we can ask for $N \in \N$ samples Throughout, we assume that the process generates i.i.d. samples. In particular, \(X_i \sim q\) for all \(i\), with \(q\) unchanging as we ask for more samples. However, we note that this is a simplification: one would imagine that an (artificial) intelligent &quot;student&quot; would judiciously ask for more informative examples from a &quot;teacher&quot; process. , $D_N = { X_1, \dots, X_N }$. Our goal is to learn a distribution $p(x)$ from which we can make inferences about the data generating process itself. In other words, the task of a statistical learning machine is to discover structures and properties $q(x)$ from training examples $D_N$. Some examples: Deterministic data: If $q$ generates the result of “$1 + 1$”, we can set $p(x_1) = 1$ where $x_1 = 2$ is the first “measurement” or request we made to the data generating process. Here the learning process recover everything we wish to know about $q$ just from the first data point, i.e. $p = q$. As such, there is no reason to deviate from this learning process." /><link rel="canonical" href="https://edmundlth.github.io/posts/introduction-to-singular-models/" /><meta property="og:url" content="https://edmundlth.github.io/posts/introduction-to-singular-models/" /><meta property="og:site_name" content="Probably Singular" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-06-01T00:00:00+10:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Introduction to Singular Learning Theory" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-12-05T22:09:57+11:00","datePublished":"2021-06-01T00:00:00+10:00","description":"Introduction Let’s first set the context. Imagine we are given a data generating process $q(x)$ where we can ask for $N \\in \\N$ samples Throughout, we assume that the process generates i.i.d. samples. In particular, \\(X_i \\sim q\\) for all \\(i\\), with \\(q\\) unchanging as we ask for more samples. However, we note that this is a simplification: one would imagine that an (artificial) intelligent &quot;student&quot; would judiciously ask for more informative examples from a &quot;teacher&quot; process. , $D_N = { X_1, \\dots, X_N }$. Our goal is to learn a distribution $p(x)$ from which we can make inferences about the data generating process itself. In other words, the task of a statistical learning machine is to discover structures and properties $q(x)$ from training examples $D_N$. Some examples: Deterministic data: If $q$ generates the result of “$1 + 1$”, we can set $p(x_1) = 1$ where $x_1 = 2$ is the first “measurement” or request we made to the data generating process. Here the learning process recover everything we wish to know about $q$ just from the first data point, i.e. $p = q$. As such, there is no reason to deviate from this learning process.","headline":"Introduction to Singular Learning Theory","mainEntityOfPage":{"@type":"WebPage","@id":"https://edmundlth.github.io/posts/introduction-to-singular-models/"},"url":"https://edmundlth.github.io/posts/introduction-to-singular-models/"}</script><title>Introduction to Singular Learning Theory | Probably Singular</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Probably Singular"><meta name="application-name" content="Probably Singular"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://d3awt3adt4koyi.cloudfront.net/images/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Probably Singular</a></div><div class="site-subtitle font-italic">Mathematics and Machine Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/edmundlth" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['edmundlth95','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Introduction to Singular Learning Theory</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <script src="/assets/js/sidenote.js"></script><div id="latex-macros" style="display:none"> $$ \newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\P}{\mathbb{P}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\V}{\mathbb{V}} \newcommand{\E}{\mathbb{E}} \newcommand{\A}{\mathbb{A}} \newcommand{\F}{\mathbb{F}} \newcommand{\L}{\mathcal{L}} \newcommand{\sgn}{\mathrm{sgn}} \newcommand{\brac}[1]{\left ( #1 \right)} \newcommand{\set}[1]{\left \{ #1 \right \}} \newcommand{\st}{\, \mid \,} \newcommand{\wh}{\,|\,} \newcommand{\abrac}[1]{\left \langle #1 \right \rangle} \newcommand{\sqbrac}[1]{\left [ #1 \right]} $$</div><h1 data-toc-skip>Introduction to Singular Learning Theory</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Edmund Lau </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Jun 1, 2021, 12:00 AM +1000" >Jun 1, 2021<i class="unloaded">2021-06-01T00:00:00+10:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Dec 5, 2021, 10:09 PM +1100" >Dec 5, 2021<i class="unloaded">2021-12-05T22:09:57+11:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1280 words">7 min read</span></div></div><div class="post-content"> <img data-proofer-ignore data-src="https://d3awt3adt4koyi.cloudfront.net/images/2021-08-01/algebraic-curve.png" class="preview-img" alt="image not found" width="300" ><h1 id="introduction">Introduction</h1><p>Let’s first set the context. Imagine we are given a data generating process $q(x)$ where we can ask for $N \in \N$ samples <span sidenote="">Throughout, we assume that the process generates i.i.d. samples. In particular, \(X_i \sim q\) for all \(i\), with \(q\) unchanging as we ask for more samples. However, we note that this is a simplification: one would imagine that an (artificial) intelligent "student" would judiciously ask for more informative examples from a "teacher" process.</span> , $D_N = { X_1, \dots, X_N }$. Our goal is to <em>learn</em> a distribution $p(x)$ from which we can make inferences about the data generating process itself. In other words, the task of a statistical learning machine is to discover structures and properties $q(x)$ from training examples $D_N$.</p><p>Some examples:</p><ul><li><p><strong>Deterministic data:</strong> If $q$ generates the result of “$1 + 1$”, we can set $p(x_1) = 1$ where $x_1 = 2$ is the first “measurement” or request we made to the data generating process. Here the learning process recover everything we wish to know about $q$ just from the first data point, i.e. $p = q$. As such, there is no reason to deviate from this learning process.</p><li><p><strong>Deterministic with measurement error</strong> If $q$ generates the results of ballot count by humans, the above learning process would still be reasonable, but we should perhaps account for human error. We could, for instance, ask for lots of recount set $p(\text{most frequently occuring count}) = 1$. Or perhaps a deterministic result doesn’t sit well with us when we know that error can occur, we can set $p(x) =$ proportion of recount that turns out to be $x$.</p><li><p><strong>Experiments in empirical science</strong> If $q$ generates experimental measurements of physical quantities $(x, y)$ that is governed by some law of nature $y = f_\alpha(x)$ that depends on some parameter $\alpha$ and experimental measurements is marred by (normally distributed) random error, then we have $Y_i - f_\alpha(X_i) \sim N(0, \sigma)$. The value of $\alpha$ can be estimated given a learnt model $p$.</p><li><p><strong>Generalised Linear Models</strong></p><li><p><strong>AI agents</strong></p><li><h2 id="etc-">etc <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">1</a></sup></h2></ul><p>In general, we instantiate a large space of <em>hypothesis</em>,</p>\[\Delta = \left\{p = p(x|w) \, \, : \,w \in W\right\}\]<p>parametrised by $w \in W \subset \R^d$ equipped with a prior $\varphi(w)$ and cast the learning process as an <em>optimisation procedure</em> that finds the best hypothesis that explains the observed samples. One way to define “best” is to select $p$ that minimises the <em>Kullback-Leibler divergence</em> between $q$ and $p$, i.e.choose $p(x) = p(x \mid \hat{w})$ such that $\hat{w}$ minimises</p>\[\begin{aligned} K(w) = \E_q\left[\log \frac{q(x)}{p(x| w)}\right] = \int_X q(x) \log \frac{q(x)}{p(x| w)}dx \end{aligned}\]<p>We will investigate the properties of learning machine of this form. Properties of a learning machine that we might care about:</p><ul><li><p>Error rate. Generalisation. Generalisation gap: \(B_g, G_g, B_t, G_t\).</p><li><p>Data efficiency. Compute efficiency. Behaviour in overparametrised regime. Scaling laws. Double descent.</p><li><p>Training behaviour. Stochastic noise.</p></ul><h1 id="real-log-canonical-threshold">Real Log Canonical Threshold</h1><p>For a given statistical model $(p(x \mid w), q(x), \varphi(w))$, the following are equivalent definitions for its real log canonical threshold (RLCT), $\lambda$ and its order $\theta$.</p><ol><li><p><strong>Largest pole of zeta function of \(K\)</strong><br /> Define the zeta function of $K$ as<span sidenote="">\(\zeta\) analytically continues to a meromorphic function with poles on the negative real axis.</span>:</p>\[\begin{aligned} &amp; \zeta: \C \to \C &amp;\zeta(z) = \int_W K(w)^z \varphi(w)dw. \end{aligned}\]<p>The RLCT $\lambda$ is the largest pole of $\zeta$ and $\theta$ the order of the pole at $\lambda$.</p><li><p><strong>Convergence rate of Laplace integral of $K$</strong><br /> $(\lambda, \theta)$ governs the asymptotic behaviour as $n \to \infty$ of the Laplace integral<span sidenote="">which is the deterministic version of the (normalised) evidence \(Z^0_n = \int_W \exp\left(-nK_N(w)\right)\varphi(w) dw\). Note that the limiting variable \(n\) is different from the number of training samples \(N\). This is one place where inverse temperature \(\beta\) can come in: set \(n = \beta k\).</span>:</p>\[\begin{aligned} \int_W \exp\left(-nK(w)\right)\varphi(w) dw \stackrel{n \to \infty} \sim Cn^{-\lambda}\left(\log n\right)^{\theta -1} \end{aligned}\]<p>for some positive real constant $C$.</p><li><p><strong>Convergence rate of free energy</strong><br /> Taking the negative logarithm of the previous asymptotic expression gives<span sidenote="">the stochastic version translate as \(F^0_n = \lambda \log n - (\theta -1) \log \log n +\) stochastic process of constant order.</span></p>\[\begin{aligned} \log \int_W \exp\left(-nK(w)\right)\varphi(w) dw \stackrel{n \to \infty} \sim \lambda \log n - \left(\theta -1\right) \log \log n + O(1). \end{aligned}\]<li><p><strong>Asymptotic expansion of density of states near $W_0$</strong><br /> The density of state</p>\[\begin{aligned} v(t) = \int_W \delta\left(t - K(w)\right) \varphi(w) dw \end{aligned}\]<p>has asymptotic expansion as $t \to 0$</p>\[\begin{aligned} v(t) \sim C t^{\lambda -1} (- \log(t))^{\theta -1} \end{aligned}\]<p>for some positive real constant $C$.</p><li><p><strong>Volume codimension $W_0$</strong></p>\[\begin{aligned} \lambda = \lim_{t \to 0^+} \log_a \frac{V(at)}{V(t)} \end{aligned}\]<p>where $1 \neq a &gt; 0$ and</p>\[V: \R_{\geq 0} \to \R_{\geq 0}\]<p>is the volume measure of neighbourhoods of $W_0$</p>\[\begin{aligned} V(t) = \int_{K(w) &lt; t} \varphi(w) dw. \end{aligned}\]<li><p><strong>From resolution of singularity</strong><br /> Hironaka’s resolution of singularity for the real analytic function $K(w)$ gives us a proper birational map<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">2</a></sup> $g: U \to W$ such that in the neighbourhood of $w_0 \in W_0$, the zero set of $K$</p>\[\begin{aligned} K(g(u) - w_0) &amp;= u^{2k} = u_1^{2k_1}u_2^{2k_2} \dots u_d^{2k_d}\\ g'(u) &amp;= b(u)u^h = b(u)u_1^{h_1}u_2^{h_2} \dots u_d^{h_d} \end{aligned}\]<p>for some $u, k \in \N^d$ and analytic $b(u) \neq 0$. We then have</p>\[\begin{aligned} \lambda = \inf_{w \in W_0} \min_{1 \leq j \leq d}\frac{h_j + 1}{2k_j} \end{aligned}\]<p>and $\theta$ is given by the number of times the above minimum is achieved.<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">3</a></sup></p><li><p><strong>RLCT of ideals of analytic functions</strong><br /> TODO: there are various square roots involved in this that I don’t really understand</p></ol><h1 id="rlct-for-regular-models">RLCT for Regular Models</h1><p>The RLCT of a regular realisable model is given by</p>\[\lambda = \frac{d}{2} \quad \theta = 1.\]<p>We shall use the Laplace integral characterisation of RLCT. We want to show that</p>\[Z^0_n = \int_W \exp\left(-nK(w)\right) \varphi(w) d{w} \sim C n^{-\frac{d}{2}}\]<p>for some positive constant \(C\) as \(n \to \infty\). Since the model is realisable and identifiable, it has unique minimum at \(w^* \in \mathrm{supp}(\varphi)\). Taylor expansion of \(K\) centered around \(w^*\) up to order 2 gives</p>\[\begin{aligned} K(w) &amp;= K(w^*) + \nabla K(w^*) \cdot (w - w^*) + \frac{1}{2} (w - w^*)^T \nabla^2K(w^*)(w - w^*) + O(\left|\, w - w^* \,\right|^3) \end{aligned}\]<p>where $\nabla^2K(w^<em>)$ is the Hessian of $K$ at $w^</em>$. That $w^<em>$ realises the true model and is a local minimum gives us $K(w^</em>) = 0$ and $\nabla K(w^*) =0$, reducing the above to</p>\[\begin{aligned} K(w) &amp;= \frac{1}{2} (w - w^*)^T \nabla^2K(w^*)(w - w^*) + O(\left|\, w - w^* \,\right|^3). \end{aligned}\]<p>Substituting the above into the integral, we get, in the limit as \(n \to \infty\)</p>\[\begin{aligned} Z^0_n &amp;\sim \int_W \exp\left(-\frac{n}{2} (w - w^*)^T \nabla^2K(w^*)(w - w^*) \right) \varphi(w) d{w} \end{aligned}\]<p>which we recognise as a Gaussian integral with precision matrix \(n \nabla^2K(w^*)\) which is positive definite by assumption. Therefore, we conclude that</p>\[\begin{aligned} Z^0_n \sim \varphi(w^*)\sqrt{\frac{(2\pi)^d}{\det\left(n \nabla^2K(w^*)\right)}} = \varphi(w^*)\sqrt{\frac{(2\pi)^d}{\det\left(\nabla^2K(w^*)\right)}} n^{-\frac{d}{2}}. \end{aligned}\]<p>We shall use the characterisation that for any positive \(a \neq 1\), \(\lambda = \lim_{t \to 0^+} \log_a \frac{V(at)}{V(t)}\) where \(V\) is the volume function</p>\[\begin{aligned} V(t) = \int_{K(w) \leq t} \varphi(w) dw. \end{aligned}\]<p>By regularity assumption, we have that \(w^*\) is a non-degenerate critical point of \(K\) and hence by Morse lemma, there is a local chart \(x(w) = (x_i(w))_{i = 1, \dots, d}\) in a small enough neighbourhood of \(w^*\) such that \(K(w) = \cancelto{0}{K(w^*)} + \sum_i x_i(w)^2\). Therefore, for small enough \(t &gt; 0\),</p>\[\begin{aligned} V(t) = \int_{\sum_i x_i^2 \leq t} \varphi(x) dx \end{aligned}\]<p>which is proportional to the volume of a \(d\)-dimensional ball with radius \(\sqrt{t}\), i.e. \(V(t) \propto t^{d}\).[^16] Finally,</p>\[\begin{aligned} \lambda = \lim_{t \to 0^+} \log_a \frac{(at)^{d/2}}{t^{d/2}} = \frac{d}{2}. \end{aligned}\]<h1 id="footnotes">Footnotes</h1><div class="footnotes" role="doc-endnotes"><ol><li id="fn:2" role="doc-endnote"><p>TODO: more examples. Contrast different inference tasks. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:7" role="doc-endnote"><p>obtained via recursive blow up. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:8" role="doc-endnote"><p>This deep result shows that \((\lambda, \theta) \in \Q \times \Z\). <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/singular-learning-theory/'>singular-learning-theory</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/singular-learning-theory/" class="post-tag no-text-decoration" >singular-learning-theory</a> <a href="/tags/algebraic-geometry/" class="post-tag no-text-decoration" >algebraic-geometry</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >machine-learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Introduction to Singular Learning Theory - Probably Singular&url=https://edmundlth.github.io/posts/introduction-to-singular-models/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Introduction to Singular Learning Theory - Probably Singular&u=https://edmundlth.github.io/posts/introduction-to-singular-models/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/statistical-learning-theory-via-a-coin-flipping-demon/">Statistical Learning Theory Via A Coin Flipping Demon</a><li><a href="/posts/singular-learning-theory-part-1/">Singular Learning Theory - Part 1</a><li><a href="/posts/hartshorne-algebraic-geometry-1.1/">Algebraic Geometry - Hartshorne 1.1</a><li><a href="/posts/introduction-to-grobner-basis/">Introduction to Grobner Basis</a><li><a href="/posts/overview-of-singular-learning-theory/">Overview of Singular Learning Theory</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/machine-learning/">machine-learning</a> <a class="post-tag" href="/tags/singular-learning-theory/">singular-learning-theory</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/seminar-notes/">seminar-notes</a> <a class="post-tag" href="/tags/algebraic-geometry/">algebraic-geometry</a> <a class="post-tag" href="/tags/grobner-basis/">grobner-basis</a> <a class="post-tag" href="/tags/hartshorne/">hartshorne</a> <a class="post-tag" href="/tags/real-log-canonical-threshold/">real-log-canonical-threshold</a></div></div></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/singular-learning-theory-part-1/"><div class="card-body"> <span class="timeago small" >Dec 28, 2021<i class="unloaded">2021-12-28T00:00:00+11:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Singular Learning Theory - Part 1</h3><div class="text-muted small"><p> “We approached the case, you remember, with an absolutely blank mind, which is always an advantage. We had formed no theories. We were simply there to observe and to draw inferences from ou...</p></div></div></a></div><div class="card"> <a href="/posts/overview-of-singular-learning-theory/"><div class="card-body"> <span class="timeago small" >Nov 15, 2021<i class="unloaded">2021-11-15T00:00:00+11:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Overview of Singular Learning Theory</h3><div class="text-muted small"><p> Singular Learning Theory (SLT), pioneered by Sumio Watanabe, acknowledges the presence of singularities in statistical models. provides a foundation grounded in Algebraic Geometry to work with...</p></div></div></a></div><div class="card"> <a href="/posts/statistical-learning-theory-via-a-coin-flipping-demon/"><div class="card-body"> <span class="timeago small" >Jan 13<i class="unloaded">2022-01-13T00:00:00+11:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Statistical Learning Theory Via A Coin Flipping Demon</h3><div class="text-muted small"><p> “It is simplicity itself, so absurdly simple that an explanation is superfluous; and yet it may serve to define the limits of observation and of deduction." Sherlock Holmes, "The Sign...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <span class="btn btn-outline-primary disabled" prompt="Older"><p>-</p></span> <a href="/posts/introduction-to-grobner-basis/" class="btn btn-outline-primary" prompt="Newer"><p>Introduction to Grobner Basis</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://edmundlth.github.io/posts/introduction-to-singular-models/'; this.page.identifier = '/posts/introduction-to-singular-models/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://probably-singular.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (modeToggle !== null) { modeToggle.addEventListener('click', reloadDisqus); window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/edmundlth">Edmund Lau</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/singular-learning-theory/">singular learning theory</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/seminar-notes/">seminar notes</a> <a class="post-tag" href="/tags/algebraic-geometry/">algebraic geometry</a> <a class="post-tag" href="/tags/grobner-basis/">grobner basis</a> <a class="post-tag" href="/tags/hartshorne/">hartshorne</a> <a class="post-tag" href="/tags/real-log-canonical-threshold/">real log canonical threshold</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
