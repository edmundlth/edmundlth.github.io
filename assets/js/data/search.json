[ { "title": "Free Energy Formula - Proof", "url": "/posts/proof-of-free-energy-formula/", "categories": "singular-learning-theory, singular-learning-theory-lecture-series", "tags": "singular-learning-theory, machine-learning, statistics, seminar-notes", "date": "2022-04-14 00:00:00 +1000", "snippet": "" }, { "title": "Standard Form of Log-loss - Proof", "url": "/posts/proof-of-standard-form-of-log-loss/", "categories": "singular-learning-theory, singular-learning-theory-lecture-series", "tags": "singular-learning-theory, machine-learning, statistics, seminar-notes", "date": "2022-04-07 00:00:00 +1000", "snippet": "" }, { "title": "Introducing Standard Form of Log-loss", "url": "/posts/introducing-standard-form-of-log-loss/", "categories": "singular-learning-theory, singular-learning-theory-lecture-series", "tags": "singular-learning-theory, machine-learning, statistics, seminar-notes", "date": "2022-03-24 00:00:00 +1100", "snippet": "" }, { "title": "Climbing Ladder to Free Energy Asymptotics", "url": "/posts/climbing-ladder-to-free-energy-asymptotics/", "categories": "singular-learning-theory, singular-learning-theory-lecture-series", "tags": "singular-learning-theory, machine-learning, statistics, seminar-notes", "date": "2022-03-17 00:00:00 +1100", "snippet": "" }, { "title": "Ladder to Free Energy Asymptotics", "url": "/posts/ladder-to-free-energy-asymptotics/", "categories": "singular-learning-theory, singular-learning-theory-lecture-series", "tags": "singular-learning-theory, machine-learning, statistics, seminar-notes", "date": "2022-02-24 00:00:00 +1100", "snippet": "" }, { "title": "Statistical Learning Theory Via A Coin Flipping Demon", "url": "/posts/statistical-learning-theory-via-a-coin-flipping-demon/", "categories": "singular-learning-theory, singular-learning-theory-lecture-series", "tags": "singular-learning-theory, machine-learning, statistics", "date": "2022-01-13 00:00:00 +1100", "snippet": " “It is simplicity itself, so absurdly simple that an explanation is superfluous; and yet it may serve to define the limits of observation and of deduction.&quot; Sherlock Holmes, &quot;The Sign of Four&quot; In this post, a simple statistical inference task of a modeling a coin flip shall be our vehicle to introduce important statistical concepts. These concepts will become central to our study in Singular Learning theory. We shall show our calculations with excruciating details in hope that such simple case will be instructive and serve to define the limits of our tools in extracting exact results. Post summary: To model the outcome of a coin flip, we introduce a general way to do statistical inference by constructing a parameterised model - a family of probability distributions, each a possible approximation to the process that generates the coin flips. We introduce the likelihood function . It gives the probability of observing the given data set assuming they are generated by the model at a specified parameter. We introduce a popular statistical estimation method, the Maximum Likelihood method, to estimate the outcome of a coin flip. With this method, the likelihood function is treated as an objective function to be maximised and the model at the maximum is used to approximate the truth. We point out that the result of such an inference procedure is itself random. Specifically, it depends on the random sampling that generates the data set. It is the role of a statistical learning theory to clarify if a certain statistical estimation procedure is to be employed What is the expected performance given subject to the randomness of training samples? For what class of true data generating process would the procedure approximate well? Using the normal approximation to the binomial distribution, we shall see that the maximum likelihood estimator for the coin flip model is asymptotically normal and asymptotically efficient two desirable properties that is the hallmark of regular model but are absent in singular models. The Situation, The Dataset Statistical Inference Models Learning and Inference The Adventure of Maximum Likelihood Estimation Empirical Log Loss and Log Likelihood Ratio Finally Some Calculations So how did we do? A Broader Theory of MLE and Fisher Information Role of Statistical Learning TheoryThe Situation, The DatasetSuppose there is a coin-flipping demon on the other side of a door. He offers us a deal. We can bet a number of years of our remaining life. If his next coin flip turns out to be head, he will pay us double what we bet. Otherwise, he keeps the bet. Rather uncharacteristicallyor maybe it’s just a deception, he offers to first perform $N$ flips and let us look at the results. We shall call this set of observations\\[D_N = \\set{X_1, X_2, \\dots, X_N}, \\quad X_i \\in \\set{H, T}\\]our dataset or training samples .As a pure utilitarian, we decide to not run. Which means we better have a really good estimate of the probability $\\hat{p}$ of the next flip turning out heads since most sound betting strategy relies on it. For instance, the Kelly’s criterion gives the optimal fraction of our life in this situation as $2 \\hat{p} - 1$ .Statistical InferenceModelsSo we are in a situation where we are given a training set, $D_N$, of previous coin flips and our task is to estimate the probability $\\hat{p}$ that the next flip comes up head.Let $h$ denote the number of heads in the dataset $D_N$ and $t = N - h$ the number of tails. Model 0. We can simply guess that it&#39;s a fair coin so that $\\hat{p} = 1/2$. This is a not a good inference method not less because a demon wouldn&#39;t be fair. There is no learning involved at all. We ignored the dataset, which should be a valuable resource in dealing with a demon. Even in this case, we are still left with questions like when exactly is this method badjust so that no one will come and say &quot;I told you so&quot; when the demon actually uses a fair coin and how to quantify exactly how bad the method is. Model 1. We can instead construct a family of hypothesis, parameterised by $w \\in [0, 1]$. Each $w$ represent a guess that the probability of landing heads $=w$. We say that we are defining a model $$ p(X \\mid w) = \\begin{cases} w, &amp;amp; X = H \\\\ 1 - w, &amp;amp; X = T \\end{cases} $$ with parameter $w$ taking value in the parameter space $W = [0, 1]$. We can now reason as follow: If the probability of landing heads were to be $w$, then the probability that the dataset $D_N$ that we observed get generated is $$ p(D_N \\mid w) = \\prod_{i = 1}^N p(X_i \\mid w) = w^h (1 - w)^t. $$ Notice that we are assuming that, on the other side of the door, the demon is consistently throwing the same coin in the same way, so that we get a constant probability of landing heads. We call this true probability of landing heads the true parameter $w_0$. This means that each $X_i$ follows an identical distribution, $p(H) = w_0$, and that each observation is independent of all others. Together, this is the common independent and identically distributed (i.i.d) assumption that allows us to factorise $p(D_N\\mid w)$ as we did aboveWe could stipulate additionally that the order of $X_i$ doesn&#39;t matter, introducing a factor of $\\binom{N}{h}$ making the above the binomial distribution. But we omit this for exposition uniformity. The binomial coefficient will be reintroduced whenever we sum over these sequences anyway. . We call the probability of observing a given dataset for a given parameter the likelihood function $$l_N(w) = p(D_N \\mid w)$$ which, to emphasise, is a function of paramter $w$ with the dataset $D_N$ fixed. Model 2. But the demon could be more ... demonic. Every time he does a flip, he could&#39;ve been choosing one of two coins at random from a box, only one of which is a fair coin. We can introduce another parameter $s \\in [0,1]$ for representing the probability that the fair coin is chosen and $w \\in [0, 1]$ represent the bias - probability of landing heads - for the other coin. We have now a two dimensionalbut still compact paramter space $(s, w) \\in [0, 1]^2$. The hypothesis at each parameter is given by $$ \\begin{align*} p(H \\mid s, w) &amp;amp;= \\frac{1}{2}s + w (1 -s) \\\\ p(T \\mid s, w) &amp;amp;= \\frac{1}{2}s + (1 - w) (1 - s) \\end{align*} $$ and the likelihood function becomes $$ l_N(w) = \\prod_{i = 1}^Np(X_i \\mid s, w) = \\brac{\\frac{1}{2}s + w (1 -s)}^h \\brac{\\frac{1}{2}s + (1 - w)(1 -s)}^t $$ which again uses the i.i.d assumption. Learning and InferenceWe could continue to imagine more and more demonic opponent on the other side, but we should first see what can the models do for us.We can continue the reasoning with likelihood function as follow: We now have $l_N$ giving us the probability of observing $D_N$ for specific parameter. Surely the most likely $w$ is the one that give the highest probability for the observed data? this reminds me a little of the antropic principle. Hence we should set the estimate $\\hat{p}$ to be the parameter $w$ that maximises $l_N(w)$.This gives us the statistical inference method known as Maximum Likelihood Estimate (MLE), i.e. find the single parameter this makes it a point estimate. $w$ that maximises the likelihood function.This is a natural line of reasoningthere was a time when I was only taught this method of statistical estimation and think this is the only way., and indeed it is in some sense the correct method for this situation.However, MLE assumed quite a few things. MLE says “surely another coin with a different probability of landing heads would produce quite different looking observations $D_N$?”. Well, not really. What if there is only one observation $N = 1$? Let’s say $D_1 = \\set{H}$. Surely a whole range of $w$ could’ve plausibly generated itas we shall see, the MLE answer in this case would be at the boundary of $[0, 1]$, $\\hat{p} = 1$ which, at least to me, doesn’t seem to be a well-justified answer.? “Ah, but if $N$ is truly huge, small difference in $w$ will result in very different $D_N$”, says MLE. We shall see that this is indeed true. As the number of observations grow, it becomes asymptotically impossible for another parameter but the true one to generate the sequence of coin flips observed. BUT, that is only true if we made the assumption that the true parameter is contained in the model! This is the realisability assumption. For instance, if the demon is really following the coin flipping recipe laid out in model 2 above, then maximising the likelihood function of model 1 how much worse is model 0?wouldn’t give us accurate answer, not even asymptotically. MLE doesn’t provide the answer to the quenstion “what is the most likely parameter given the observation” $= p(w \\mid D_N)$, which is a distinct question to “what is the most likely observation given the parameter” $= p(D_N \\mid w)$.The Adventure of Maximum Likelihood EstimationDespite the above misgiving, we shall push through with MLE of model 1. We shall have explicit computation and introduce several important statistical concepts along the way. We shall see in what sense is MLE a good estimator in this case and what could go wrong in other situations where we might like to use the same procedure.We shall also make the realisability assumption where the demon’s coin flip follows the following true distribution\\[\\begin{align*} q(H) &amp;amp;= w_0 \\\\ q(T) &amp;amp;= (1 - w_0)\\end{align*}\\]for some true paramter $w_0 \\in [0, 1]$. Meaning $q(x) = p(x \\mid w_0)$.Empirical Log Loss and Log Likelihood RatioWe wish to set $\\hat{p} =$ the maximum of the likelihood function\\[l_N(w) = w^h (1 - w)^t.\\]If $N$ is a large number, the above expression produces really small numbers which can be numerically challenging to handle. We usually do a monotonic hence doesn’t change any extrema of the function transform by taking its logarithm, which also happily simplifies several calculations. In fact, we shall introduce the empirical log loss function\\[L_N(w) = -\\frac{1}{N} \\log p(D_N \\mid w) = -\\frac{1}{N} \\sum_{i = 1}^N \\log p(X_i \\mid w)\\]which we shall aim to minimisehence the usage of the term “loss”.. In our case, we get\\[L_N(w) = \\frac{1}{N} \\brac{\\sum_{X_i = H} \\log w + \\sum_{X_i = T} \\log (1 - w)} = \\frac{1}{N} \\sqbrac{h \\log w + t \\log (1 - w)}\\]Below is a little tangent justifying the introduction of this quantity and its connection to the log likelihood ratio function which will be an important information theoretic quantity for our study. Connection to log likelihood ratio The Shanon entropy of the true distribution is given by $$ S(q) = -\\E_q(\\log q(X)) = -q(H)\\log q(H) + -q(T) \\log q(T) $$ which can be approximated by the empirical entropy using the observation from the data set $D_N$ $$ S_N(q) = -\\frac{1}{N} \\sum_{i = 1}^N \\log q(X_i). $$ Adding this quantity to the empirical log loss function we get the empirical log likelihood ratio $$ K_N(w) = \\frac{1}{N} \\sum_{i = 1}^N \\log \\frac{q(X_i)}{p(X_i \\mid w)} = -\\frac{1}{N} \\sum_{i = 1}^N \\log p(X_i \\mid w) + \\frac{1}{N} \\sum_{i = 1}^N \\log q(X_i) = L_N(w) - S_N(q) $$ which is the empirical estimate to the central object of study, the expected log likelihood rato better known as the Kullback-Leibler divergence of the model at $w$ to the true distribution $$ K(w) = D_{KL}(q(x) \\mid \\mid p(x \\mid w)) = \\E_X\\sqbrac{\\log \\frac{q(X)}{p(X \\mid w)}}. $$ We note that minimising $K_N(w)$ is equivalent to minimising $L_N(w)$ since they differ by an additive constant that is independent of $w$. Finally Some CalculationsSince our objective function $L_N(w)$ is a differentiable function, we shall use the stationary point criterion for finding local minima.So, we find that maximimum likelihood estimate is given by\\[\\hat{p} = \\hat{w} = \\frac{h}{N}.\\]It’s important to emphasise again that $h$ in the expression above is the number of heads in the given data set $D_N$, it depends on the given set of observations so is itself a random variable!indeed, even functions we defined like $l_N(w)$, $L_N(w)$, $K_N(w)$ are all random variables, they just take values in the space of functions. This is why proper investigation into their behaviour in statistical learning theory requires functional analysis. So how did we do?Though we now have our estimate, there is still a concern that we might have gotten an atypical data set which makes our guess wildly inaccurate. We need some way to quantify the uncertainty of our $\\hat{w} = h / N$ estimate. Fortunately, we can do some exact calculation in this case.The asymptotic normality $\\hat{w} \\to N(Nw_0, Nw_0 (1 - w_0))$ uses the normal approximation to binomial distribution. can be proven by apply the Central Limit Theorem to sums of Bernoulli random variables or directly use a classic result of de Moivre-Laplace.The array of plots below shows the normal density superimposed on histograms of $\\hat{w}$ for different values of $N$ and $w_0$. Observe that the normal approximation works better for large values of $N$ and away from pathological values of $w_0 = 0, 1$. The closer the true parameter $w_0$ is to the boundary, the large the data set needs to be for the normal distribution to correctly approximate the density away from the meanImagine $w_0$ being close to the boundary, then when $N$ is small the variance $w_0(1 - w_0) / N$ is large, which makes a large portion of the normal density extends in to the “imppossible” region $ \\hat{w} \\not\\in [0, 1]$. .A Broader Theory of MLE and Fisher InformationHaving been forced by the demon to go through these exercises in statistical inference we noticed a few things about MLE for model 1: The maximum likelihood parameter exist and is unique, even when it turns out to be at the boundary $\\hat{w} = 0, 1$. It is an unbiased estimator, meaning the expected MLE prediction gives the true paramter (again, assuming realisability.) It is asymptotically normal, meaning as the number of observations $N \\to \\infty$, the distribution of $\\hat{w}$ approaches that of the normal distribution. It turns out that the form the of asymptotic normal distribution implies that $\\hat{w}$ achieves asymptotic efficiency , meaning it achieves the lowest variance possible for any unbiased estimator as prescribed by the Cramer-Rao inequality.Before we state the Cramer-Rao bound, we need a few preliminaries. It turns out that the above is actually true for MLE of a large class of models $p(x \\mid w)$ modeling some true distribution $q(x)$ satisfying some regularity conditions which turns out to be far too strong for general statistical learning theory. Hence the need to study “Singular Models”. .Even with slightly more complicated models, it is unlikely that we will have the luxury of computing the estimator distribution exactly as we did in this simple case. However, taking limits ($N \\to \\infty$) is always a good way to wash away the details.Perhaps the most famous among the tools for washing away details is the Central Limit Theorem (Central Limit Theorem) Given a sequence of i.i.d. random variables $\\set{X_1, X_2, \\dots}$, each with finite means and variance, $\\mu = \\E[X_i] &amp;lt; \\infty$ and $0 &amp;lt; \\sigma^2 = \\V[X_i] &amp;lt; \\infty$, we can take partial averages $S_n = \\frac{1}{n} \\sum_{i = 1}^n X_i$. Then, as $n \\to \\infty$, $S_n$ converges in distribution to $N(\\mu, \\sigma^2/ \\sqrt{n})$, $$ \\sqrt{n}(S_n - \\mu) \\overset{d}{\\to} N(0, \\sigma^2). $$ meaning the probability density of $\\sqrt{n}(S_n - \\mu)$ converges pointwise at points of continuity. see here. to that of the normal density $$ \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{\\sigma^2}}. $$So, if somehow our estimators are sums of increasingly numerous i.i.d. random variables, we have asymptotic normality. Just like in the case of the coin flip where the number of heads $h$ is given by sums of i.i.d. random variables $\\sim Bernoulli(w_0)$. We might not have that in general. However, it turns out that the spread of MLE estimator is related to the following object. For a given statistical model $p(x \\mid w)$ with $x \\in \\R^n$ and $w \\in \\R^d$, the Fisher information matrix is given by $$ I(w) = \\mathrm{CovariantMatrix}(\\frac{\\partial}{\\partial w_j}\\log p(x \\mid w)) $$ where the covariance is taken with $p(x \\mid w)$ itself as probability distribution. Explicitly in component form, $$ I_{jk}(w) = \\int \\brac{\\frac{\\partial}{\\partial w_j}\\log p(x \\mid w)}\\brac{\\frac{\\partial}{\\partial w_k}\\log p(x \\mid w)} p(x \\mid w) dx. $$In the 1D case, this reduces to a scalar function\\[I(w) = \\V_{X \\sim p(x \\mid w)}(\\frac{\\partial}{\\partial w}\\log p(X \\mid w)) = \\int \\brac{\\frac{\\partial}{\\partial w}\\log p(x \\mid w)}^2 p(x \\mid w) dx.\\]We interpret this as answering the following question:If the data were sampled from the distribution at $w$, $X_i \\sim p(x \\mid w)$, what is the variance of the $\\frac{d}{dw} \\log p(x \\mid w)$, the log-likelihood ratio. Recall that we use the condition that $\\frac{d}{dw} \\log p(x \\mid w)$ to find (local) maximum of the likelihood function which we then take as our estimate of the true parameter. So, informally, $I(w)$ is related to the spread of our MLE prediction.Computing this quantity for our coin toss model,we get\\[I(w) = \\frac{1}{w(1 - w)}.\\]The lemma proven above shows that $I(w) = \\E_{X \\sim p(x \\mid w)} \\sqbrac{-\\partial^2_w \\log p(x \\mid w)}$. This gives another intuition of what $I(w)$ measures: If the data were sampled from the distribution at $w$, what is the expected curvature of the log-likelihood near $w$? Said another way: If the true parameter $w_0$ were to be at $w$, what is the expected curvature of the log-likelihood near the true parameter? From the calculation, at the true parameter $I(w_0) = (n \\sigma^2)^{-1}$ where $\\sigma^2 = w_0(1 - w_0) / N$ is the variance of $\\hat{w}$ computed before. This relationship between estimator variance and Fisher information is not a coincidence. It is evidence that MLE for our model above is asymptotically efficient. This means that the $\\sigma^2$ is the tightest asymptotic spread of our estimator possible. Put another way, our MLE achieves the Cramer-Rao lower bound stated below. (Cramer-Rao bound) Let $\\hat{w}$ is an unbiased estimator for a model $p(x \\mid w)$ computed from a set of i.i.d. samples $\\set{X_1, \\dots X_N}$, $X_i \\sim p(x \\mid w)$. Let$V_{ij} = \\E\\sqbrac{(\\hat{w}_i - w_i)(\\hat{w}_j - w_j)}$ be the covariance matrix and $I_{ij}(w)$ the Fisher information matrix for the model. Then $$ V \\geq \\brac{N I(w)}^{-1} $$ where inequality of matrices is defined as $A \\leq B \\iff A - B$ is positive semidefinite. The theorem says that the Fisher information matrix give the highest precision or the lowest possible variance precision $= 1 / $ variance any unbiased estimator can achieve. In fact, this is more general than just the MLE for our simple coin toss case. (MLE&#39;s are asymptotically efficient) Let $p(x \\mid w)$, $w \\in W \\subset \\R^d$ be a parametric model. Let $D_N = \\set{X_1, \\dots, X_N}$ be a dataset of size $N$ with i.i.d. samples from the probability density at a true parameter $w_0$, $X_i \\sim p(x \\mid w_0)$ with $w_0$ being in the interior of $W$. Under some regularity conditions, the maximum likelihood estimator is asymptotically normal and efficient: $$ \\sqrt{N} \\brac{\\hat{w}_{MLE} - w_0} \\overset{d}{\\to} N(0, I(w_0)^-1) $$ where $I(w)$ is the Fisher information matrix for the model. Some regularity conditions: It is rather complex to carefully state the regularity conditions here. For rigorous discussions, see (Lehmann, 2005, chapter 12). But the conditions frequently involves $W$ is an open subset. The model is identifiable. Meaning distinct parameters gives distinct probability density. The density $p(x \\mid w)$ for any $w$ has common support. $p(x \\mid w)$ is thrice continuously differentiable with respect to $w$. The model has positive definite Fisher information matrix. Also, notice that the lower bound does not apply when $I(w) = 0$ such as when $w = 0, 1$ in our coin toss model and more generally when $I_{ij}(w)$ is not an invertible matrix.Role of Statistical Learning TheoryThe role of statistical learning theory is to clarify the performance of procedures like MLE we used above. What if it is not realisable? This is a rather common case for modeling complex processes. What if the true parameter is at a singularity of the model? Nearby points are affected as well. What happen when $N$ is small? Lehmann, E. L., and Joseph P. Romano. 2005. Testing Statistical Hypotheses. Springer, New York, NY. " }, { "title": "Algebraic Geometry - Hartshorne 1.1", "url": "/posts/hartshorne-algebraic-geometry-1.1/", "categories": "algebraic-geometry", "tags": "hartshorne, algebraic-geometry", "date": "2022-01-03 00:00:00 +1100", "snippet": "This is a series on studying Algebraic Geometry based on the textbook “Algebraic Geometry” by Hartshorne. We start off with the so called “classical algebraic geometry” where the unifying concept of schemes has yet to be introduced. We will define the geometric objects we shall be studying known as algebraic varieties and associate with them algebraic objects where by geometric properties can be extracted via algebraic means. A Little Motivation Definition of Affine VarietiesA Little MotivationIt wasn’t until a little while after learning a little geometry and algebra past undergraduate level that it finally hits me: “$x^2 + y^2 = 1$” is the circle well… with lots mathematical preamble suppressed. But that’s what the definitions in this section is for., not just a mere representation or tool to manipulate the set of points on the circle. Not only that, the way – the algebraic way – it is presented lends itself to various obvious generalisations that are “correct” read: plays well with intuitions and other mathematical theories. For instance, definitions higher dimensional analogs is immediate with “$x_1^2 + x_2^2 + \\dots + x_n^2 = 1$” and, with perhaps more historical significant, the study of conic section can be unified under the study of general degree 2 polynomials\\[ax^2 + bxy + cy^2 + dx + ey + f = 0.\\]Similarly, most students of mathematics need no convincing when it comes to the power of talking about points, lines, planes and hyperplanes and so on in the language of linear algebra.We shall now define the rather general setting of algebraic varieties to study such association between algebra and geometry.Definition of Affine VarietiesLet’s fixed a field $k$. Unless otherwise specified, we shall assume that $k$ is algebraically closedwe shall see why when discuss the celebrated Hilbert Nullstellenstaz. But, with apology, we will be drawing pictures as though $k = \\R$..Let $n \\in \\N$. We shall call the set of all $n$-tuples of elements in the field $k$ the $n$-dimensional affine space or the affine $n$-space and denote it as $$ \\A_k^n := k^n = \\set{(a_1, a_2, \\dots, a_n) \\wh a_i \\in k \\text{ for all } i}$$We call any element $P = (a_1, \\dots, a_n) \\in \\A_k^n$ of the space a point with the $a_i$&#39;s being its coordinates . This should remind us of the familiar case of cartesian coordinates $(x, y) \\in \\R^2$ used above to defined the circle $x^2 + y^2 = 1$. We are simply generalising this with arbitrary field $k$meaning we would like to retain the ability to add, multiple, divide and have access to $0$ and $1$. and to arbitrary dimensions. This is the algebraic stage where our geometric objects shall abide.Let $k[x_1, \\dots, x_n]$ denote the ring of polynomials in $n$ variables over $k$. We naturally interpret each polynomial $f \\in k[x_1, \\dots, x_n]$ as a function $A^n_k \\to k$ $$P = (a_1, \\dots, a_n) \\mapsto f(P) = f(a_1, \\dots, a_n)$$The preimage of zero of such a polynomial $f$ is called the zero set or the vanishing set hence the notation $\\V$ for &quot;vanishing&quot; of the function$$\\V(f) = \\set{(a_1, \\dots, a_n) \\in \\A_k^n \\st f(a_1, \\dots, a_n) = 0}.$$We can also talk about the _common vanishing set_ of a set of functions $F \\subset k[x_1, \\dots, x_n]$ $$\\V(F) = \\bigcap_{f \\in F}\\V(f) = \\set{P \\in \\A^n_k \\st f(P) = 0 \\text{ for all } f \\in F}$$The set of points in $\\A^n_k$ that corresponds to precisely such vanishing sets are called algebraic sets $$\\begin{align*}X \\subset \\A^n_k &amp;amp;\\text{ is an algebraic set } \\\\&amp;amp;\\iff \\\\X = \\V(F) &amp;amp;\\text{ for some } F \\subset k[x_1, \\dots, x_n].\\end{align*}$$Now, given such a set of polynomials $F \\subset k[x_1, \\dots, x_n]$, we can consider $\\abrac{F}$, the ideal generated by $F$\\[I = \\abrac{F} = \\set{\\sum_{i = 1}^k a_i(x)f_i(x) \\st k \\in \\N, f_i \\in F, a_i \\in k[x_1, \\dots, x_n]}\\]Observe that if a point $P \\in A^n_k$ is a zero for all polynomials in $F$, then it is a zero for all polynomials in $I$ too. Conversely, since $F \\subset I$, a common zero of polynomials in $I$ is a common zeros for those in $F$. We conclude,\\[\\V(F) = \\V(\\abrac{F}) = \\V(I).\\]We shall henceforth always consider the ideal $\\abrac{F}$ instead of just $F$ itself. Moreover, by Hilbert Basis Theorem, $k[x_1, \\dots, x_n]$ is a noetherian ring a ring if noetherian if all its ideals are finitely generated., even if the initial family of polynomial $F$ is infinite, there exist a finite set $\\set{g_1, \\dots, g_r}$ of generators of $\\abrac{F}$ so that we can express the algebraic set as\\[\\V(F) = \\V(\\abrac{F}) = \\V(g_1, \\dots, g_r).\\]" }, { "title": "Singular Learning Theory - Part 1", "url": "/posts/singular-learning-theory-part-1/", "categories": "singular-learning-theory, singular-learning-theory-lecture-series", "tags": "singular-learning-theory, algebraic-geometry, machine-learning", "date": "2021-12-28 00:00:00 +1100", "snippet": " “We approached the case, you remember, with an absolutely blank mind, which is always an advantage. We had formed no theories. We were simply there to observe and to draw inferences from our observations.” Sherlock Holmes, &quot;The Adventure of The Cardboard Box&quot; This is intended as the first of a series of articles going through core texts for Singular Learning Theory (henceforth SLT) - &quot;Algebraic Geometry and Statistical Learning Theory&quot;(Watanabe, 2009) and &quot;Mathematical Theory of Bayesian Statistics&quot; (Watanabe 2018) both written by Sumio Watanabe. This is perhaps mainly my attempt at digesting the text not for the first time disguised as a series of lectures. The target audience I have in mind is anyone familiar with undergraduate level mathematics and is interested in theoretical / mathematical aspect of statistical learning and AI. That being said, the nature of the subject is such that it draws upon tools and concepts from a wide array of mathematical disciplines, spanning algebra to analysis. Some, like probability and statistics, are crucial in the sense that they are the objects of study. Some, like manifold theory, are only required to make sure that the mathematical objects we manipulate are well defined and cover a sufficient generality for the theory to be useful. Others, like algebraic geometry and Schwartz distribution theory, exports crucial theorems that we shall use to prove and understand the central results of SLT. Yet others, like statistical mechanics, are topics where we might find unexpected connections and possible cross-pollination. We shall introduce these topics in their own time when they come up naturally when we explore SLT. Our modest aim regarding these prerequisites is to understand them with sufficient depth to understand the proofs of various results in SLT and to at least understand their significance like why they are needed and what happen when we can&#39;t borrow from them. Of course each of them are profound fields of study in their own right, and if time and energy permit, we shall delve beyond strictly necessary to see the wonder they contain. What do we aspire to study? Statistical Background The Task of Learning Model-Truth-Prior Some Important Statistical Estimation Methods Kullback-Leibler Divergence Theory of Statistical Learning Fisher Information Matrix Singular Models Model Selection MISC ReferencesWhat do we aspire to study?Before making definitions and carving out our domain, let’s list a few informal ways we think and talk about our field of study If nothing else, it will help with triangulating what we aspire to study, where the definitions and tools we shall develop are helpful and where there are not. . We are studying systems that can learn from examples. We are studying algorithms or physical devices that perceive the world and learn its patterns. We are studying ways to reason and make decision with imperfect information and in the presence of randomness. We are studying models of systems with randomness and how to find the best approximation for the particular part of the world under observation. We are studying, or at least trying to study what we think is, intelligence. In particular, machine intelligence, of how machine can learn patterns and make intelligent decisions. Examples of objects of study that falls under these aspirations. A device that implement the calculation $\\sum_{i = 1}^6 f(i)/6$ can make decision on whether a given payout $f$ of a game using a six-sided dice is worth playing. Perhaps a better, more intelligent, device / machine would actually uses observations on 10000 dice throw and implement $\\sum_{i = 1}^6 f(i) \\frac{\\mathrm{freq}(i)}{10000}$ instead. A statistician can use a sample of human height measurements $\\set{h_1, \\dots, h_n}$ and estimate the distribution of heights of comparable human population, or to make a good guess on the probability that a child would grow to be taller than 150cm. The natural language model GPT-3 is a massive and complicated approximation to human languages. AlphaGo is a machine intelligence that were able to learn from millions of generated examples of games of Go, and learn the pattern of winning strategies so well that it was able to defeat the reigning world champion. Brains of human and other intelligent species seems to implement complicated and as yet poorly understood system that learn the pattern of their surroundings, able to triage tasks and resources and make decisions critical to the survival of its species. Statistical BackgroundThe Task of LearningLet’s come back down to earth with some definitions. In typical statistical learning scenario, we are given a set of examples coming from a data generating process. We are given a data set $$ D_N = \\set{X_1, \\dots, X_N} $$ of $N \\in \\N$ training samples consisting of independently and identically distributed random variables $X_i$ i.e. i.i.d. samples drawn from what we shall call the true distribution specified by a probability density $q(x)$.Unless otherwise specified, the random variables take values in a fixed Euclidean space $\\R^n$. For clarity, we shall often have rather strong regularity assumptions on any probability distributions that occurs throughout the development of SLT. Not only do we assume that densities exist for probability distributions we study, we shall also assume at least continuity and likely analyticity full set of conditions will be specified later on. This setting allow us to leverage heavy duty machineries from algebraic geometry to explicate salient features of learning machines before persuing further generality. Though this seems rather restrictive, it does cover a large class of distributions we encounter in practice like normal mixtures, neural networks, Bayesian networks and much more. The truth $q$ is what a learning machine wants to access, but can only see the shadow of via the data set $D_N$. For our purpose, we shall formulate the task of a statistical learning machine as one that approximates the true distribution $q$ from the given set of training samples $D_N$.A statistical inference or a learning machine is a measurable function taking any data set $D_N$ to probability distribution again, we shall work with distribution on $\\R^n$. specified by $\\hat{p}(x)$, $$\\begin{align*}\\mathrm{LearningMachine}: \\set{\\text{data set of any size $N$}} &amp;amp;\\to \\set{\\text{probability distributions}} \\\\ D_N &amp;amp;\\mapsto \\hat{p}(x).\\end{align*} $$Among the concerns of a statistical learning theory includes studying how to construct such a learning machine how to obtain $\\hat{p}$ efficiently efficiency in terms of computation resources and size of data set. and how well does $\\hat{p}$ thus constructed approximates $q$.Model-Truth-PriorA useful and general way of constructing and studying the approximation $\\hat{p}$ to a given truth $q(x)$ is to first looking at a large family of distributions, have a way of measuring how well each of them approximates $q$, then come up with a procedure to find the best performing approximation. A statistical model we might sometimes call this the space of hypothesis is a family of probability distribution $p(x \\mid w)$ parameterised by a subset of $d$-dimensional parameter space $W \\subset \\R^d$ for some $d \\in \\N$. We assume that $p(x \\mid w)$ at all $w \\in W$ have the same support as the true distribution $q(x)$, $$ \\overline{\\set{x \\in \\R^n \\wh p(x | w) &amp;gt; 0}} = \\overline{\\set{x \\in \\R^n \\wh q(x) &amp;gt; 0}}. $$ Given a set of i.i.d. training samples $D_N = \\set{X_1, \\dots, X_N}$ as before, we call probability of observing $D_N$ for a particular parameter $$ L_N(w) := \\P(X_1, \\dots, X_N \\mid w) = \\prod_{i = 1}^N p(X_i \\mid w) $$ the likelihood function. Taking a Bayesian perspective, we shall endow the parameter space $W$ with a prior probability distribution with density $\\varphi(w)$ so that, applying Bayes Theorem, we get the posterior distribution on $W$ $$ p(w \\mid D_N) = \\frac{L_N(w) \\varphi(w)}{Z_N} $$ where the normalising constant in the denominator $$ Z_N := \\int_W L_N(w) \\varphi(w) dw = \\int_W \\prod_{i = 1}^N p(X_i \\mid w) \\varphi(w) dw $$ is an important quantity known as the model evidence. (Quite) a few things to say about $Z_N$ This quantity comes with a few different namestestament to its importance and ubiquity in mathematical sciences.. In Bayesian statistics, it goes by the names &quot;model evidence&quot; or &quot;marginal likelihood&quot;. In statistical mechanics, it goes by &quot;partition function&quot; or, when taken a log-transform, the &quot;free energy&quot;, denoted as $$ F_N = -\\log Z_N $$ To justify its name, we observe that $Z_N$ depends on the set of random variables $D_N$, hence it is itself a random variable. In fact, it is precisely the probability of observing the data $\\set{X_1, \\dots, X_N}$ given the model and prior. Perhaps it is clearer in the basic form of Bayes Theorem $P(H|X) = P(X|H) P(H) / P(X)$ thinking of $H$ as a particular hypothesis in a class of hypotheses and $X$ as the observed data. The denominator $P(X) = \\sum_{h \\in \\text{all hypothesis}} P(X|h)P(h)$ is the probability of observing $X$, $P(X)$, given the hypothesis class. It is there for the likelihood of the given model-prior pair given the data, hence &quot;model evidence&quot;. It is computed by marginalise out the model parameter $w$, hence &quot;marginal likelihood&quot;. We shall also consider a generalisation of the posterior distribution and model evidence where introduce a continuous parameter $\\beta &amp;gt; 0$ which we shall interpret as &quot;inverse temperature&quot; $\\beta = \\frac{1}{T}$: $$ \\begin{align*} p(w \\mid D_N) &amp;amp;= \\frac{\\varphi(w) \\prod_{i = 1}^N p(X_i \\mid w)^\\beta}{Z_N(\\beta)}\\\\ Z_N(\\beta) &amp;amp;= \\int_W \\varphi(w) \\prod_{i = 1}^N p(X_i \\mid w)^\\beta dw. \\end{align*} $$ Note that $\\beta = 1$ reduces down to the case of usual Bayes posterior distribution and evidence. As $\\beta \\to \\infty$, the posterior distribution converges the delta distribution concentrated at the maximum likelihood estimate of $w$, $\\delta(w - \\hat{w}_{ML})$, if it exist and is unique in the support of $\\varphi(w)$. We can think of this case as taking the temperature down to zero and having the system considers only the ground state specified by the observed data. As $\\beta \\to zero$, the posterior collapsed to the prior. This is the case where we have infinite temperature and observed data has no effect on the system exploring all allowed state at the frequency specified by the prior. To make the analogy to statistical mechanics even tighter, we can introduce the analog to the potential field by considering quantities normalised by the true probabilities of observed data $q(X_i)$. $$ \\begin{align*} Z^0_N(\\beta) &amp;amp;= \\frac{Z_N(\\beta)}{\\prod_{i = 1}^N q(X_i)^\\beta} \\\\ F^0_N(\\beta) &amp;amp;= -\\log Z^0_N(\\beta) = F_N(\\beta) - N\\beta S_N \\\\ S_N &amp;amp;= -\\frac{1}{N} \\sum_{i = 1}^N \\log(q(X_i)) \\end{align*} $$ where $S_N$ is the empirical entropy. &quot;Empirical&quot; because it is an empirical estimate of the entropy of the true distribution given by $$ S(q) = - \\int q(x) \\log q(x) dx = \\E[-\\log q(x)] = \\E[S_N]. $$ Some Important Statistical Estimation MethodsWith the above objects defined, we can now specify a few ways to construct $\\hat{p}$.First, there is the maximum likelihood method that constructs an approximation to the truth $q$ by finding, among the model parameter space $W$, the parameter $w \\in W$ that maximises the likelihood function. The assumption is that, since the data set $D_N$ is generated from $q(x)$, a good approximation of $q$ is the model $p(x \\mid w)$ that have the highest likelihood to have generated the same data. The Maximum Likelihood Estimator (ML) is given by the model parameter that maximises the likelihood function. $$ \\hat{w}_{ML} = \\mathrm{argmax}_{w \\in W} L_N(w) $$ It turns out that it is easier in practice for numerical reasons and more convenient for theoretical discussions to introduce the negative log-likelihood function $l_N(w) = -log L_N$ and the MLE is given by minimising $l_N(w)$, i.e. $\\hat{w}_{MLE} = \\mathrm{argmin}_{w \\in W} l_N(w)$. With the MLE, the approximate distribution is given by the model at $\\hat{w}_{MLE}$, $$ \\hat{p}_{ML}(x) = p(x \\mid \\hat{w}_{ML}). $$ MLE is a common point estimation method. Meaning an objective function, the likelihood function in this case, is constructed and then an optimisation procedure is employed to find the optimum which gives the desired estimate. It is often assumed that the model space is parametrised in a such a way that the likelihood function has desirable functional properties suitable for optimisation such as differentiability, convexity or, like in the case of large neural networks, convenient form to compute gradient for gradient descent.Bringing some Bayesian perspective into the picture, we can construct another point estimate by biasing the likelihood function with prior belief of what parameters $w \\in W$ are a priori more likely to occur without having observe any data. This gives us the Maximum a Posteriori method. The Maximum a Posteriori (MAP) estimate is obtained by maximising the posterior distribution $$ \\hat{w}_{MAP} = \\mathrm{argmax}_{w \\in W} p(w \\mid D_N) = \\mathrm{argmin}_{w \\in W} l_N(w) - \\log \\varphi(w) $$ where $\\varphi(w)$ is the prior distribution. The approximation to $q(x)$ is then given by $$ \\hat{p}_{MAP}(x) = p(x \\mid \\hat{w}_{MAP}). $$ In extreme cases, if we believeon sound theoretical grounds or just sheer human pig-headedness that the model parameters lies in a certain region and nowhere else, then even if the MLE method claim that the data is most likely to be generated by the model at $\\hat{w}_{MLE}$, the MAP estimate might completely disregard that possibility.Instead of considering one “best” parameter in the model, a fully Bayesian perspective will advocate for taking distributions at all parameters $w \\in W$ into account, with the importance of each $p(x \\mid w)$ weighted by their posterior probability. The Bayes predictive distribution for the approximation of $q(x)$ is given by $$ \\hat{p}_{B} = p(x | D_N) = \\E_{w}\\sqbrac{p(x \\mid w)} = \\int_{W} p(x \\mid w) p(w \\mid D_N) dw $$ where $\\E_w$ denote taking expectation with respect to the posterior. Another method of constructing statistical estimation $\\hat{p}$ perhaps rare in practice but holds theoretical importance is that of Gibbs estimation. With Gibbs estimation, every timeI am a little unsure about whether we draw $\\hat{w}$ once and for all or perform multiple draws as described. one wish to estimate the probability of $X = x$, one starts by randomly drawing a parameter $\\hat{w}$ from the posterior distribution $p(w \\mid D_N)$ and answer with $p(X = x \\mid \\hat{w})$. The Gibbs estimation method is given by randomly drawing a parameter from the posterior distribution $$ \\hat{w}_{gibbs} ~ p(w \\mid D_N) $$ and give the approximation to $q(x)$ by $$ \\hat{p}_{gibbs} = p(x \\mid \\hat{w}_{gibbs}). $$ Kullback-Leibler DivergenceNext come the problem of how to measure the success or failure of our approximation. We need a quantity that measures the difference between two density functions. Enter the Kullback-Leibler divergence. The Kullback-Leibler Divergence or KL-divergence between two probability density functions $q(x)$ and $p(x)$ on an open set $A \\subset \\R^n$ is given by $$ K(q \\mid\\mid p) := \\int_A q(x) \\log \\frac{q(x)}{p(x)} dx. $$ KL-divergence on general probability measure KL-divergence can be defined in much greater generality between any two probability measures $P$ and $Q$ over the same measure space $(X, \\sigma)$ by $$ K(Q \\mid\\mid P) := \\int_X \\log \\frac{dQ}{dP} dQ $$ whenever the Radon-Nikodymn derivative $\\frac{dQ}{dP}$ exists i.e. $Q$ is absolutely continuous with respect to $P$.. If there is another measure $\\mu$ on $X$ for which $p = \\frac{dP}{d\\mu}$ and $q = \\frac{dQ}{d\\mu}$ exists, then by the chain rule, we recover a more familiar form $$ K(Q \\mid\\mid P) = \\int_X \\log\\brac{\\frac{\\frac{dQ}{d\\mu}}{\\frac{dP}{d\\mu}}} \\frac{dQ}{d\\mu} d\\mu = \\int_X q \\log \\frac{q}{p} d\\mu. $$ In the definition we gave above, we have been using the standard Lebesgue measure on $\\R^n$. As another example, if we have the counting measure $\\mu_c$ on a discrete measure space $X$ and probability measures $P$ and $Q$ on $X$, then the expression becomes, $$ K(Q \\mid\\mid P) = \\sum_{x \\in X}Q(x) \\log \\frac{Q(x)}{P(x)}. $$ I find it helpful to understand a little about Shanon’s information entropyand we shall need the concept of information entropy in our study anyway. in order to get an intutitive sense of KL-divergence. Informally, Shanon’s information quantifies how surprised one should be when an event that was thought to have probability $p \\in [0, 1]$ occurs. Intuitively, the observation “the sun rose in the east this morning” has less information content than “it was raining this morning in Perth” or “the Dow Jones index rose by 3% this morning”. Think about which “insider knowledge” - knowledge that dispels prior uncertainty by increasing the event probability to 1 - will you pay more to obtain to give an edge in a bet. Stated another way, the information content quantifies ignorance about an event prior to observation.A little more formally, Shanon information is another way of encoding probability with the following properties Information content is a monotonically decreasing non-negative function $I: [0, 1] \\to \\R_{\\geq 0}$ of the probability $p$ of the event. Events with complete certainty have zero information. Information of independent events adds. These properties together with some smoothness assumptions on $I$ forces\\[I(p) = -\\log p.\\]If we have a set of events following a probability distributionmore precisely, a sigma algebra of events with a probability measure, i.e. a probability space, we can quantify the expected amount of information, or the expected amount of surprise if we were to observe a large amount events drawn from the distribution. See Shanon’s original 1948 paper (Shanon, 1948) that started off information theory for formal derivation and discussion. The Shanon entropy of a random variable $X$ taking on finite number of values ${x_1, \\dots, x_n}$ each with probability $p_i = p(X = x_i)$ is given by the following expectation expressionwe use the letter $S$ for entropy here, consistent with statistical physics. However, Shanon originally uses the letter $H$ for this quantity, named after Boltzmann&#39;s H-theorem for thermodynamics. $$ S(p) = \\E_p[-\\log p(X)] = -\\sum_{x \\in X} p(x) \\log p(x) $$ For continuous distribution $p(x)$, we replace the sum with integral $$ S(p) = \\int p(x) \\log p(x) dx. $$ Observe that even though low probability events has high information content $-\\log p(x)$, they occur with less frequency $p(x)$. This push and pull, together with the fact that probability sum to unity, means that the distribution with the highest entropy is the uniform distributionif there is no other constraints and if a uniform distribution can be defined on the space. Theory of Statistical Learning\\(-\\frac{1}{N} \\log L_N(w) \\to K(w) - S(q)\\)But minimisation of $-\\log L_N(w)$ is not equivalent to minimisation of $K(w)$ (which would give the closest estimate to $q(x)$). So MLE is not the only valid or the best statistical estimation method.Fisher Information MatrixSingular ModelsVarious equivalent definitions of singular models (via FIM and K(w)). Some implications.Model SelectionMISCSeminar blackboard draftReferences Sumio Watanabe. 2009. &quot;Algebraic Geometry and Statistical Learning Theory.&quot; Cambridge University Press. Sumio Watanabe. 2018, &quot;Mathematical Theory of Bayesian Statistics.&quot; CRC Press, Chapman and Hall/CRC. Shannon, C. E. 1948. “A Mathematical Theory of Communication.” The Bell System Technical Journal 27 (3): 379–423." }, { "title": "Overview of Singular Learning Theory", "url": "/posts/overview-of-singular-learning-theory/", "categories": "singular-learning-theory", "tags": "singular-learning-theory, real-log-canonical-threshold, statistics, machine-learning", "date": "2021-11-15 00:00:00 +1100", "snippet": "Singular Learning Theory (SLT), pioneered by Sumio Watanabe, acknowledges the presence of singularities in statistical models. provides a foundation grounded in Algebraic Geometry to work with singularities. clarifies foundational results in probability and statistics when regularity assumptions are violated. studies how properties of model singularities and other algebro-geometric properties affect properties of statistical models.Table of Content The Task of Learning The Mathematics of Learning Kullback-Leibler Divergence Generalisation Gap Model Selection Singular Learning TheoryThe Task of LearningThe task of a statistical learning machine is to discover structures and properties of a data generating process $X$ with probability density $q(x)$ from a set of examples (training data) $D_N = \\set{X_1, \\dots, X_N}$ identically and independently drawn from the process $X_i \\sim q(x)$.Indeed, in hopes of recovering the truth $q$ itself, we define a statistical model, which a family of probability distributions $p(x \\mid w)$, parametrised by $d$-dimensional real parameters $w \\in W \\subset \\R^d$ endowed with a prior probability density $\\varphi(w)$. Given such model-truth-prior triplet, a learning machine\\[D_N \\mapsto p(x|\\hat{w})\\]follows specified algorithmic procedure to search for its best guess $\\hat{w}$ or a family of guesses based on information it can glean from examples in $D_N$, so that they can be used to approximate $q(x)$.Maximum Likelihood Estimation (MLE) is a well-known example of such a procedure where a single $\\hat{w}$ is found by maximising the average log-likelihood function $L_N(w) = \\frac{1}{N}\\sum_{i} \\log p(X_i \\mid w)$ and $q$ is approximated by $p(x \\mid \\hat{w})$. One can also first apply Bayes Theorem to obtain a posterior distribution\\[p(w\\|D_N) = \\frac{e^{NL_N(w)} \\varphi(w)}{\\int_W e^{NL_N(w)} \\varphi(w)dw}\\]and set $\\hat{w}$ to be its maximum, a.k.a. maximum a posteriori (MAP) estimate. Of particular theoretical significance is the Bayesian estimate, where no single $\\hat{w}$ is trusted, instead, the truth $q$ is estimated by the posterior average over $w$, i.e.\\[p^*(x) = \\int_W p(x \\mid w) p(w \\mid D_N) dw.\\]As a concrete example of contemporary interest and to illustrate the scale at which modern big data and big models, the Generative Pre-trained Transformer 3 (GPT-3) is a deep neural network model with $d \\sim 175$ billion parameters, trained using $N &amp;gt;$ 400 billion natural language tokens scraped from the Web. The put emphasis on the algorithmic or “machine” aspect of learning, GPT-3 is trained with a variant of the workhorse of modern machine learning: Stochastic Gradient Descent, an iterative algorithm where a better $\\hat{w}$ is proposed by taking a small step in the opposite direction of the gradient $\\nabla_w \\L(\\hat{w}_{\\text{old}})$ of a loss function $\\L$. The efficient computation of gradient on a models with large $d$ and $N$ with backpropagation is itself a significant algorithmic breakthrough.The Mathematics of LearningHere we sketch several central mathematical objects that arise from the study of statistical learning machines described above and briefly discuss how singularities arise to necessitate the need for methods from algebraic geometry.Kullback-Leibler DivergenceFirst, a quantity is needed to represent how well each probability density $p(x|w)$ in the model approximate the truth $q(x)$. For this, we use the Kullback-Leibler Divergence or relative entropy as established in information theory\\[K(w) := D_{KL}(q \\|\\| p) = \\int q(x) \\log \\frac{q(x)}{p(x\\|w)} dx. \\label{kl-div}\\]It is a non-negative quantity $K(w) \\geq 0$ for any $w$ and a density $p(x|\\hat{w}) = q(x)$ almost everywhere if and only if $K(\\hat{w}) = 0$. Put another way, $K(w) = \\E_X[\\log q(x) - \\log p(x|w)]$, it is the generalisation error as it expresses the expected difference between (log of) the truth $q$ with a selected model $p(x|w)$. When the expection $\\E_X$ is estimated using training data, we get the empirical version of $K$\\[K_N(w) := \\frac{1}{N} \\sum_{i = 1}^N \\log \\frac{q(X_i)}{p(X_i\\|w)}\\]also known as the training error. Since, $\\E_X[K_N(w)] = K(w)$, as the size $N$ of training data increases, $K_N(w)$ converges pointwise almost surely to $K(w)$ by law of large numbers. Observe that, the quantity above can be rewritten as\\[K_N(w) = -L_N(w) + \\frac{1}{N}\\sum_{i = 1}^N \\log q(X_i)\\]where the second term is the empirical version of the entropy $S(q)$ of $q$. Since only the first term has $w$ dependence, maximising the log-likelihood $L_N(w)$ – as per MLE – minimises $K_N(w)$. Unfortunately, as Watanabe pointed out in \\cite{Watanabe2009-mg}, minimising $K_N(w)$ does not translate minimising $K(w)$ even with $N \\to \\infty$ since the following limiting operations do not commute\\[\\E_X[\\min_w K_N(w)] \\neq \\min_w \\E_X[K_N(w)] = \\min_w K(w).\\]Therein lies the tragedy and opportunity of statistical learning theory. The hope for a single best estimation method is dashed but we are promised a rich theory where we shall study the various mode of convergence $K_N \\to K$ in function space of differing topology and their correspondence to different learning machines.Generalisation GapThis naturally leads to the study of generalisation gap $K_N - K$. Since this quantity is a measurable function of the data set $D_N$, i.e. a random variable, we asked for uncertainty quantification and also for its rate of decay as $N \\to \\infty$ which quantifies “data efficiency”: how large a data set do we need to collect before we are confident with our theoretical guarantees.Observe that, for $w$ where $K(w) &amp;gt; 0$, $K_N(w)$ has finite strictly positive variance, $\\sigma^2 &amp;gt; 0$, we can invoke central limit theorem to get the following pointwise convergence in law to a normal distribution\\[\\sqrt{N}\\brac{K_N(w) - K(w)} \\xrightarrow{d} N(0, \\sigma^2). \\label{eq:clt-empirical}\\]Unfortunately, it is not estimates far from $q$ that interest us, nor does pointwise convergence reveals much about the behaviour of the generalisation gap in the neighbourhood of $\\set{w \\wh K(w) = 0}$. As our main interest is to approximate the truth $q$, behaviour around neighbourhood of $\\set{K(w) = 0}$ constitute the main theme of our study.Here’s where we see that the geometry of the set of true parameters $W_0 = \\set{w \\in W \\wh K(w) = 0}$ become important. Aside from the case of \\emph{regular statistical models} where $W_0$ is a singleton set, $K(w) = 0$ forms an analytic variety in $\\R^d$ in the general case of \\emph{singular models}.%TODO: Picture of singularity in K = 0Model SelectionAnother question of great theoretical and practical importance is: How do we determine the relative merits of one model over another? That is, with only training samples $D_N$ and no access to the truth $q$, how do we know that a model $p_1(x|w)$ is better or worse than another $p_2(x|w)$? This can be quantified using model evidence,\\[Z_N = \\int_W \\prod_{i = 1}^N p(X_i\\|w) \\varphi(w) dw = \\int_W e^{N L_N(w)} \\varphi(w) dw\\]which is marginalises $w$, giving the probability of observing the training samples $X_i$ in $D_N$ under the model $p(x|w)$. It can be shown that this quantity is controlled by the following integral of Laplace form involving the generalisation error $K(w)$. If there is a unique $\\hat{w}$ that minimises $K$, $K(\\hat{w}) = 0$, and that the Hessian $\\nabla^2K(w)$ or equivalently the Fisher information matrix $I(w){ij} = - \\E_X[\\p{w_i}\\p_{w_j}\\log p(X|w)]$ has full rank at $\\hat{w}$, we can obtain asymptotic expansion as $n \\to \\infty$ using Laplace’s method\\[\\int_W e^{-n K(w)} \\varphi(w) dw \\sim \\sqrt{\\frac{2\\pi}{\\det \\brac{n\\nabla^2K(\\hat{w})}}}\\varphi(\\hat{w}) = O\\brac{n^{-\\frac{d}{2}}} \\label{eq:laplace}\\]where $d$ is dimension of the parameter space $W \\subset \\R^d$. That the dominant order is $\\sim n^{-d/2}$ give rise to the famous Bayesian Information Criterion (BIC).\\However, here again we are foiled by the possibility that $K(w) = 0$ might not be a singleton set, but an analytic variety with complicated geometry blocking the use of Laplace method for approximating the integral in its neighbourhood.Singular Learning TheoryAs we observed above, classical results in elementary probability and statistics are frequently frustrated when the statistical model violates the following regularity conditions: map from parameter to density $w \\mapsto p(x|w)$ is one-to-one. the Fisher information matrix $I(w)$ of the model is positive definite for all $w \\in W$.Statistical models that violates the above assumptions are called (strictly) singular models. With singular models, lots of day-to-day statistical intuitions, tools and results no longer apply, for instance Cramer-Rao bound does not hold. Maximum-likelihood estimator is not longer asymptotically Gaussian. BIC and AIC (Akaike Information Criterion) are no longer approximations of what they claim to approximate.Worse still, in machine learning and many real world applications of statistics, singular models are the norm, not the exception [Watanabe2007]. In light of this, a new foundation for singular statistical model is needed and it hinges on the following remarkable result from Algebraic Geometry.Theorem (Hironaka’s resolution of singularities)[Watanabe2009]Let $f: W \\to \\R$ be a non-constant real analytic function from a open neighbourhood of $W$ of the origin in $\\R^d$ with $f(0) = 0$. Then there exist a resolution map $g: U \\to W$ such that $U$ is a $d$-dimensional real analytic manifold. $g$ is real analytic. $g$ is a proper map. with $W_0 = \\set{w \\in W \\wh f(w) = 0}$ and $U_0 = g^{-1}(W_0)$, $g: U \\setminus U_0 \\to W \\setminus W_0$ is an analytic isomorphism. if $p \\in U_0$, there is a local chart $u = (u_1, \\dots, u_d)$ with $p$ as the origin such that $f(g(u))$ and $g’(u)$ are of the form \\(f(g(u)) = S u_1^{k_1}u_2^{k_2}\\dots u_{d}^{k_d} \\\\ g&#39;(u) = b(u) u_1^{h_1}u_2^{h_2} \\dots u_{d}^{h_d}\\) where $S \\in \\set{+1, -1}$ is a sign, $b(u) \\neq 0$ is real analytic and $k_i, h_i$ are nonnegative integers.To illustrate how such a transformation might help with singular models, we note that, in the desingularised coordinates using the resolution map $g: U \\to W$ above, we can extend the result to\\[\\sqrt{N}\\frac{K_N(g(u)) - K(g(u))}{\\sqrt{K(g(u))}} \\xrightarrow{d} \\xi(u)\\]where $\\xi$ is a Gaussian process. And the asymptotic expansion above can now be done for singular models with\\[\\log \\int_W e^{-n K(w)} \\varphi(w) dw \\sim \\lambda \\log n - (m-1) \\log \\log n + o(\\log \\log n)\\]where $\\lambda$, instead of $d/2$ in regular models, is a birational invariant of the variety $K(w) = 0$ known as reallog canonical threshold which can be characterised in many ways as summarised below:Proposition [Watanabe2009-mg, lin-phdthesis]For a given statistical model $(p(x|\\omega), q(x), \\varphi(\\omega))$, the following are equivalent definitions for its real log canonical threshold (RLCT), $\\lambda$ and its order $\\theta$. Largest pole of zeta function of $K$ Define the zeta function $\\zeta: \\C \\to \\C$ of $K$ as\\footnote{$\\zeta$ analytically continues to a meromorphic function with poles on the negative real axis.}\\[\\zeta(z) = \\int_\\Omega K(w)^z \\varphi(\\omega)d\\omega.\\] The RLCT $\\lambda$ is the largest pole of $\\zeta$ and $\\theta$ the order of the pole at $\\lambda$. Convergence rate of Laplace integral of $K$ $(\\lambda, \\theta)$ governs the asymptotic behaviour as $n \\to \\infty$ of the Laplace integral\\footnote{which is the deterministic version of the (normalised) evidence $Z^0n = \\int\\Omega \\exp\\brac{-nK_N(\\omega)}\\varphi(\\omega) d\\omega$. Note that the limiting variable $n$ is different from the number of training samples $N$. This is one place where inverse temperature $\\beta$ can come in: set $n = \\beta k$.}:\\[\\int_\\Omega \\exp\\brac{-nK(\\omega)}\\varphi(\\omega) d\\omega \\stackrel{n \\to \\infty} \\sim Cn^{-\\lambda}\\brac{\\log n}^{\\theta -1}\\] for some positive real constant $C$. Convergence rate of free energy Taking the negative logarithm of the previous asymptotic expression gives\\footnote{the stochastic version translate as $F^0_n = \\lambda \\log n - (\\theta -1) \\log \\log n + $ stochastic process of constant order. }\\[\\log \\int_\\Omega \\exp\\brac{-nK(\\omega)}\\varphi(\\omega) d\\omega \\stackrel{n \\to \\infty} \\sim \\lambda \\log n - \\brac{\\theta -1} \\log \\log n + O(1).\\] Asymptotic expansion of density of states near $W_0$ The density of state \\(v(t) = \\int_\\Omega \\delta\\brac{t - K(\\omega)} \\varphi(\\omega) d\\omega\\) has asymptotic expansion as $t \\to 0$ \\(v(t) \\sim C t^{\\lambda -1} (- \\log(t))^{\\theta -1}\\) for some positive real constant $C$.\\footnote{lots of fixing and clarification needed…Mellin transform’s involved.} Volume codimension $W_0$\\[\\lambda = \\lim_{t \\to 0^+} \\log_a \\frac{V(at)}{V(t)}\\] where $1 \\neq a &amp;gt; 0$ and $V: \\R_{\\geq 0} \\to \\R_{\\geq 0}$ is the volume measure of neighbourhoods of $W_0$\\[V(t) = \\int_{K(w) &amp;lt; t} \\varphi(\\omega) d\\omega.\\] From resolution of singularity Hironaka’s resolution of singularity for the real analytic function $K(\\omega)$ gives us a proper birational map\\footnote{obtained via recursive blow up.} $g: U \\to \\Omega$ such that in the neighbourhood of $\\omega_0 \\in W_0$, the zero set of $K$\\[\\begin{align*} K(g(u) - \\omega_0) &amp;amp;= u^{2k} = u_1^{2k_1}u_2^{2k_2} \\dots u_d^{2k_d}\\\\ g&#39;(u) &amp;amp;= b(u)u^h = b(u)u_1^{h_1}u_2^{h_2} \\dots u_d^{h_d} \\end{align*}\\] for some $u, k \\in \\N^d$ and analytic $b(u) \\neq 0$. We then have\\[\\lambda = \\inf_{\\omega \\in W_0} \\min_{1 \\leq j \\leq d}\\frac{h_j + 1}{2k_j}\\] and $\\theta$ is given by the number of times the above minimum is achieved. This deep result shows that $(\\lambda, \\theta) \\in \\Q \\times \\Z$. " }, { "title": "Introduction to Grobner Basis", "url": "/posts/introduction-to-grobner-basis/", "categories": "algebraic-geometry", "tags": "grobner-basis", "date": "2021-08-01 00:00:00 +1000", "snippet": "AbstractShort description of the post. Abstract Introduction Special Cases Degree $=1$ but Multivariable Single Variable but Degree $&amp;gt;1$ Generalising Long Division Leading Monomial Hold Center Stage Monomially Generated Ideals Gröbner Basis Buchberger Algorithm for Constructing Gröbner Basis Equivalent Characterisations of Gröbner Basis FootnotesIntroductionWhen an algebraic variety occurs in the wild, it is frequently given by the common vanishing set of a collection of polynomials. That is, we have a possibly infinite set of polynomials $\\set{f_1(x), f_2(x), \\dots} \\subset k[x_1, \\dots, x_d]$\\(k[x_1, \\dots, x_d]\\) is the ring of polynomials in $d$ variables with coefficients in a field $k$ which we mostly assume to be $\\R$ or $\\C$. We sometimes write $k[x]$ when it is clear that $x = (x_1, \\dots, x_n)$. , and our variety $X$ is given by\\[X = \\V\\brac{\\{f_i\\}_i} := \\set{f_1(x) = 0 \\text{ and } f_2(x) = 0, \\text{ and } \\dots}.\\]And really, it is the ideal generated by the set of polynomials that matters not the polynomials themselves. The given set of polynomials should be treated as the generators of the ideal, \\(I = \\abrac{f_1, f_2, \\dots}\\). We then write the same algebraic variety as\\[X = \\V\\brac{I} = \\V\\brac{\\abrac{f_1, f_2, \\dots}}\\]If we wish to do computation with varieties and their associated ideals, it would be nice to be able to do the following: Reduce to finite descriptionOutside of toy examples, the initial description of the variety probably have infinite number of generators $f_i$. It would be nice if we can reduce to a cleverly selected finite set of generators $g_i$ that encode the same ideal. This also begs the question: given another set of generators \\(\\brac{g_i}\\), how can we tell with finite computation if they describe the same ideal? Check ideal membershipGiven another polynomial $p \\in k[x]$, can we check that $p$ is in fact in the ideal $I$? Well, we need to check that $p$ is a finitecombination of the generators, $p = \\sum_{i} q_i f_i$ with polynomial coefficients $q_i \\in k[x]$. How do we find those? Identify variety from idealGiven the ideal, \\(I = \\abrac{f_i}\\) how do we actually solve the system of polynomial equations, $f_1(x) = f_2(x) = \\dots = 0$, to obtain the points in the variety. Identify ideal from varietyIf the variety is instead given by a (rationally) parametrised geometric object, can we recover the ideal from the parametrisation? For example, the unit circle in $\\R^2$ is parametrised by \\begin{equation} t \\mapsto \\brac{\\frac{1 - t^2}{1 + t^2}, \\frac{2t}{1 + t^2}}. \\end{equation}Can we recover the defining polynomial $x^2 + y^2 = 1$ from the parametrisation above? Special CasesIt turns out that the solution to the above problems for special cases correspond to familiar concepts and algorithms. Specifically, $f_i$ are polynomials of degree 1 with $d \\geq 1$ variables, meaning they are linear equations in $d$ unknowns, then methods in linear algebra applies. Specifically, we can use Gaussian Elimination algorithm to reduce the system of equations into a convenient normal form. $f_i$ are polynomials of degree $\\geq 1$ in only $d=1$ variable, then we can use Euclidean division, again, to reduce the problem into a normal form. As we shall see, algorithms for the special cases above fails in the general case in specific ways and fixing them give rise to Grobner basis.Degree $=1$ but MultivariableHere, we are given $n$ linear polynomialsmore precisely degree $=1$ polynomials in $k[x_1, x_2, \\dots, x_d]$\\[\\begin{aligned} f_1(x_1, \\dots, x_d) &amp;amp;= a_{11}x_1 + a_{12} x_2 + \\dots a_{1d} x_d + c_1 = 0 \\\\ f_2(x_1, \\dots, x_d) &amp;amp;= a_{21}x_1 + a_{22} x_2 + \\dots a_{2d} x_d + c_2 = 0 \\\\ &amp;amp;\\vdots \\\\ f_n(x_1, \\dots, x_d) &amp;amp;= a_{n1}x_1 + a_{n2} x_2 + \\dots a_{nd} x_d + c_n = 0\\end{aligned}\\]and the task is to find their vanishing set \\(\\V\\brac{f_1, \\dots, f_n}\\). Here, we can bring to bear the full weight of linear algebra. In particular, we have the famous Gaussian elimination procedure.Lets do an example,\\[\\begin{aligned} f_1 &amp;amp;= 2x + 3y - z = 0\\\\ f_2 &amp;amp;= x + y -1 = 0\\\\ f_3 &amp;amp;= x + z -3 = 0\\end{aligned}\\]Using gaussian elimination to solve the special case of varieties defined by polynomials of degree 1Single Variable but Degree $&amp;gt;1$In this case, we have $n$ single variable polynomials in $k[x]$, each having arbitrary degree\\[\\begin{aligned} f_1(x) &amp;amp;= a_{10} + a_{11}x + \\dots + a_{1d_1}x^{d_1}\\\\ f_2(x) &amp;amp;= a_{20} + a_{21}x + \\dots + a_{2d_2}x^{d_2}\\\\ &amp;amp;\\vdots\\\\ f_n(x) &amp;amp;= a_{n0} + a_{n1}x + \\dots + a_{nd_n}x^{d_n}.\\\\\\end{aligned}\\]Fortunately, for $k$ a field, $k[x]$ is an Euclidean domaininformally, an integral domain where the Euclidean algorithm make sense and works. and therefore, by a standard result in algebra, a principal domain, meaning the ideal \\(I = \\left\\langle f_1, \\dots, f_n \\right\\rangle\\) is actually generated by a single $g \\in I$. Indeed, the ideal is generated by any element with the lowest degree in $I$. One way to compute a generator is to simply perform the eponymous Euclidean algorithm for finding $gcd(f_1, \\dots, f_n)$.Example,\\[\\begin{aligned} f_1 &amp;amp;= x^4 + x^2 - x -1 \\\\ f_2 &amp;amp;= x^3 -1\\end{aligned}\\]Using Euclidean long division repeatedly to find the greatest common divisor between two polynomialsWe can then conclude that the ideal $\\abrac{f_1, f_2}$ is simply the ideal generated by their gcd, that is\\[\\abrac{f_1, f_2} = \\abrac{x -1}\\]This means that the variety is simply given by\\[\\V(f_1, f_2) = \\V(x -1) = \\set{x \\in \\R \\st x -1 = 0} = \\set{1}.\\]This description is (slightly) easier than the original for identifying points on the variety:\\[\\V(f_1, f_2) = \\set{x \\in \\R \\mid x^4 + x^2 -x - 1 = 0 \\text{ and } x^3 - 1 = 0}.\\]Generalising Long DivisionAs we saw above, an essential feature of the single variable case that makes the Euclidean algorithm work is the ability to do long division. Put it another way, given polynomial $p$ and polynomial divisor $f$ , we can find the quotient polynomial $q$ such that\\[p(x) = q(x)f(x) + r(x)\\]with $r(x)$ having lower degree than the divisor $f$.To generalise this to multivariate polynomials, we seek to generalise the above in two ways: we allow multivariate polynomials $p \\in k[x_1, \\dots, x_d]$ and we are allow to divide by a finite set of polynomials $f_1, \\dots f_n$. Meaning want to find a set of quotients $q_1, \\dots, q_n$ and a possibly zero polynomial remainder $r$ such that \\begin{aligned} p = q_1 f_1 + \\dots + q_n f_n + r \\end{aligned} where we enforce that $r$ cannot in some sense be further divided by any of the $f_i$.The generalisation is rather straight forward. However, as we shall see, it loses some desirable properties that we rely on in the two special cases we investigated.Example, TODO….Leading Monomial Hold Center StageThe last example is particularly galling. Getting zero remainderimmediately give us an expression of $p = q_1f_1 + \\dots + q_nf_n$ whichis a witness that \\(p \\in \\left\\langle f_1, \\dots, f_n \\right\\rangle\\).However, whether we end up with zero remainder or not depend on how weorder the generators. Worse still, sometimes the division produces&quot;intermediate&quot; remainder terms. In either case, even ifthere is a little magic happening here where we suddenly restrict ourselves to just long division of element of the ideal. Turns out solving this case is enough though. $p \\in I$,whether we get zero remainder is a matter of luck and the issue seems tobe that the algorithm run into a blocker: The leading term of someintermediate remainder ${\\mathrm{LT}}(r)$ is no longerdivisible by any leading term $\\mathrm{LT}(f_i)$ of thegenerating polynomials. This is the root of all evil.To rephrase, we have the scenario that some combination $\\sum q_i f_i$of the generators produces remainderdifference of elements of the ideal is in the ideal., $r = p - \\sum q_i f_i \\in I$,but $\\mathrm{LT}(r)$ is no longer divisible by any of$\\mathrm{LT}(f_i)$, i.e.$\\mathrm{LT}(r)$ is not in the ideal generated by$\\mathrm{LT}(f_1), \\dots, \\mathrm{LT}(f_n)$.Varying $q_i$, the expression of the form $p - \\sum q_i f_i$ is all of$I$, so our issue is thatnote that \\(\\left\\langle \\mathrm{LT}(f_1),\\mathrm{LT}(f_2), \\dots \\right\\rangle \\subset \\left\\langle \\mathrm{LT}(I) \\right\\rangle\\) follows trivially from definition. It is the reverse inclusion that is causing grief. \\(\\begin{aligned} \\left\\langle \\mathrm{LT}(f_1),\\mathrm{LT}(f_2), \\dots \\right\\rangle \\neq \\left\\langle \\mathrm{LT}(I) \\right\\rangle \\end{aligned}\\)where\\(\\mathrm{LT}(I) = \\left\\{\\mathrm{LT}(p) \\, \\, : \\,p \\in I\\right\\}\\).Since it is really the ideal that we care about, not the givengenerators per se, our quest to solve the problems pose at the beginningwould be resolved if we can find another set of generatorsAlso note this second jedi mind trick where I switch to using notation to $\\mathrm{LT}(f_1), \\dots$ suggesting potentially infinite generating set. We are hoping that $G$ turns out to be finite regardless of the initial size of \\(\\left\\{f_i\\right\\}\\).$G \\subset I$ such that we do have\\[\\left\\langle \\mathrm{LT}(G) \\right\\rangle = \\left\\langle \\mathrm{LT}(I) \\right\\rangle.\\]Monomially Generated IdealsOur problem motivate the study of ideals that are generated by allleading terms of $p \\in I$. These generating sets are monomials.An ideal $I \\subset k[x_1, \\dots, x_d]$ is monomially generated[@Cox2013-xd] calls it monomial ideals, but I personally remember this name better. ifit has a (potentially infinite) generating set consist of onlymonomials.That is\\(\\begin{aligned} I = \\left\\langle x^\\alpha \\, \\, : \\,\\alpha \\in A \\right\\rangle\\end{aligned}\\) for some multiindex set $A \\subset \\N^d$.Fortunately for us, it turns out that the membership problem formonomially generated ideals is simpler.A monomial $x^\\beta$ is in a monomially generated ideal\\(I = \\left\\langle x^\\alpha \\, \\, : \\,\\alpha \\in A \\right\\rangle\\) if andonly if $x^\\alpha$ divides $x^\\beta$ for some $\\alpha \\in A$.Furthermore, any polynomial $p$ is in $I$ if and only if all of itsmonomial is in $I$.[@Cox2013-xd Chapter 2.4, Lemma 2]In other words, we can test ideal membership of a monomial by testing if$\\beta \\geq \\alpha$ for some $\\alpha \\in A$ with the lexicographicordering on $\\N^d$.A main result here is thatMonomially generated ideals are finitely generated by monomials.Furthermore, the finite generating set can be chosen from any monomialgenerating set. [@Cox2013-xd Chapter 2.4, Theorem 5]This is more or less the Hilbert basis theorem for monomially generatedideals and we shall see that this result extends directly to the fullHilbert basis theorem.Gröbner BasisWe are very close to our AH HA! moment. Our problem is that theleading terms of the original generators $f_i$ does not generate\\(\\left\\langle \\mathrm{LT}(I) \\right\\rangle\\). But\\(\\left\\langle \\mathrm{LT}(I) \\right\\rangle\\) is amonomially generated ideal and Dickson’s lemma says that it is actuallyfinitely generated by some\\(\\mathrm{LT}(g_1), \\dots, \\mathrm{LT}(g_m)\\), $g_i \\in I$. In other words,\\(\\begin{aligned} \\left\\langle \\mathrm{LT}(g_1), \\dots,\\mathrm{LT}(g_m) \\right\\rangle = \\left\\langle \\mathrm{LT}(I) \\right\\rangle.\\end{aligned}\\)All we need is that these $g_i$’s will themselves, with their trailingterms and all, generate all of $I$. Well, they do!Let $I \\subset k[x_1, \\dots, x_d]$ be any ideal. Let$g_1, \\dots, g_m \\in I$ be such that\\(\\left\\langle \\mathrm{LT}(g_1), \\dots,\\mathrm{LT}(g_m) \\right\\rangle = \\left\\langle \\mathrm{LT}(I) \\right\\rangle\\),then \\(\\left\\{g_1, \\dots, g_m\\right\\}\\) generates $I$.In honour of such nice property, we give such generating sets a name.A generating set \\(\\left\\{g_1, \\dots, g_m\\right\\} \\subset I\\) of any ideal$I$ of $k[x_1, \\dots, x_d]$ with the property that\\[\\left\\langle \\mathrm{LT}(g_1), \\dots,\\mathrm{LT}(g_m) \\right\\rangle = \\left\\langle \\mathrm{LT}(I) \\right\\rangle\\]is called a Gröbner basismore a Gröbner generating set really. It doesn’t have the unique coefficient property like a basis of vector space..Buchberger Algorithm for Constructing Gröbner BasisEquivalent Characterisations of Gröbner BasisLet $k$ be any field and $I$ be an ideal of the polynomial ring$k[x_1, \\dots, x_d]$ generated by \\(G = \\left\\{g_1, \\dots, g_n\\right\\}\\).The following are equivalentin somewhat increasing order of &quot;finiteness&quot;. $G$ is a Gröbner basis for $I$. for all $p \\in k[x_1, \\dots, x_d]$, the remainder of $p$ divided by$G$ is unique. division of $p$ by $G$ does not generate “intermediate remainder” atany stage of the algorithm. $p \\in I$ if and only if $p \\equiv 0$ mod $G$. \\(\\left\\langle \\mathrm{LT}(G) \\right\\rangle = \\left\\langle \\mathrm{LT}(I) \\right\\rangle\\). for all $p \\in I$,\\(\\mathrm{LT}(p) \\in \\left\\langle \\mathrm{LT}(G) \\right\\rangle\\). for all $p \\in I$, $\\mathrm{LT}(g_i)$ divides$LT(p)$ for some $g_i \\in G$. $G$ satisfies Buchberger’s criterion. Footnotes" }, { "title": "Introduction to Singular Learning Theory", "url": "/posts/introduction-to-singular-models/", "categories": "singular-learning-theory", "tags": "singular-learning-theory, algebraic-geometry, machine-learning", "date": "2021-06-01 00:00:00 +1000", "snippet": "IntroductionLet’s first set the context. Imagine we are given a data generating process $q(x)$ where we can ask for $N \\in \\N$ samplesThroughout, we assume that the process generates i.i.d. samples. In particular, \\(X_i \\sim q\\) for all \\(i\\), with \\(q\\) unchanging as we ask for more samples. However, we note that this is a simplification: one would imagine that an (artificial) intelligent &quot;student&quot; would judiciously ask for more informative examples from a &quot;teacher&quot; process., $D_N = { X_1, \\dots, X_N }$. Our goal is to learn a distribution $p(x)$ from which we can make inferences about the data generating process itself. In other words, the task of a statistical learning machine is to discover structures and properties $q(x)$ from training examples $D_N$.Some examples: Deterministic data: If $q$ generates the result of “$1 + 1$”, we can set $p(x_1) = 1$ where $x_1 = 2$ is the first “measurement” or request we made to the data generating process. Here the learning process recover everything we wish to know about $q$ just from the first data point, i.e. $p = q$. As such, there is no reason to deviate from this learning process. Deterministic with measurement errorIf $q$ generates the results of ballot count by humans, the above learning process would still be reasonable, but we should perhaps account for human error. We could, for instance, ask for lots of recount set $p(\\text{most frequently occuring count}) = 1$. Or perhaps a deterministic result doesn’t sit well with us when we know that error can occur, we can set $p(x) =$ proportion of recount thatturns out to be $x$. Experiments in empirical scienceIf $q$ generates experimental measurements of physical quantities $(x, y)$ that is governed by some law of nature $y = f_\\alpha(x)$ that depends on some parameter $\\alpha$ and experimental measurements is marred by (normally distributed) random error, then we have $Y_i - f_\\alpha(X_i) \\sim N(0, \\sigma)$. The value of $\\alpha$ can be estimated given a learnt model $p$. Generalised Linear Models AI agents etc 1 In general, we instantiate a large space of hypothesis,\\[\\Delta = \\left\\{p = p(x|w) \\, \\, : \\,w \\in W\\right\\}\\]parametrised by $w \\in W \\subset \\R^d$ equipped with a prior $\\varphi(w)$ and cast the learning process as an optimisation procedure that finds the best hypothesis that explains the observed samples. One way to define “best” is to select $p$ that minimises the Kullback-Leibler divergence between $q$ and $p$, i.e.choose $p(x) = p(x \\mid \\hat{w})$ such that $\\hat{w}$ minimises\\[\\begin{aligned} K(w) = \\E_q\\left[\\log \\frac{q(x)}{p(x| w)}\\right] = \\int_X q(x) \\log \\frac{q(x)}{p(x| w)}dx\\end{aligned}\\]We will investigate the properties of learning machine of this form. Properties of a learning machine that we might care about: Error rate. Generalisation. Generalisation gap:\\(B_g, G_g, B_t, G_t\\). Data efficiency. Compute efficiency. Behaviour in overparametrisedregime. Scaling laws. Double descent. Training behaviour. Stochastic noise. Real Log Canonical ThresholdFor a given statistical model $(p(x \\mid w), q(x), \\varphi(w))$, the following are equivalent definitions for its real log canonical threshold (RLCT), $\\lambda$ and its order $\\theta$. Largest pole of zeta function of \\(K\\)Define the zeta function of $K$ as\\(\\zeta\\) analytically continues to a meromorphic function with poles on the negative real axis.:\\[\\begin{aligned} &amp;amp; \\zeta: \\C \\to \\C &amp;amp;\\zeta(z) = \\int_W K(w)^z \\varphi(w)dw. \\end{aligned}\\] The RLCT $\\lambda$ is the largest pole of $\\zeta$ and $\\theta$ the order of the pole at $\\lambda$. Convergence rate of Laplace integral of $K$$(\\lambda, \\theta)$ governs the asymptotic behaviour as $n \\to \\infty$ of the Laplace integralwhich is the deterministic version of the (normalised) evidence \\(Z^0_n = \\int_W \\exp\\left(-nK_N(w)\\right)\\varphi(w) dw\\). Note that the limiting variable \\(n\\) is different from the number of training samples \\(N\\). This is one place where inverse temperature \\(\\beta\\) can come in: set \\(n = \\beta k\\).:\\[\\begin{aligned} \\int_W \\exp\\left(-nK(w)\\right)\\varphi(w) dw \\stackrel{n \\to \\infty} \\sim Cn^{-\\lambda}\\left(\\log n\\right)^{\\theta -1}\\end{aligned}\\] for some positive real constant $C$. Convergence rate of free energyTaking the negative logarithm of the previous asymptotic expression givesthe stochastic version translate as\\(F^0_n = \\lambda \\log n - (\\theta -1) \\log \\log n +\\) stochasticprocess of constant order.\\[\\begin{aligned} \\log \\int_W \\exp\\left(-nK(w)\\right)\\varphi(w) dw \\stackrel{n \\to \\infty} \\sim \\lambda \\log n - \\left(\\theta -1\\right) \\log \\log n + O(1). \\end{aligned}\\] Asymptotic expansion of density of states near $W_0$The density of state\\[\\begin{aligned} v(t) = \\int_W \\delta\\left(t - K(w)\\right) \\varphi(w) dw\\end{aligned}\\] has asymptotic expansion as $t \\to 0$\\[\\begin{aligned} v(t) \\sim C t^{\\lambda -1} (- \\log(t))^{\\theta -1} \\end{aligned}\\] for some positive real constant $C$. Volume codimension $W_0$\\[\\begin{aligned} \\lambda = \\lim_{t \\to 0^+} \\log_a \\frac{V(at)}{V(t)}\\end{aligned}\\] where $1 \\neq a &amp;gt; 0$ and\\[V: \\R_{\\geq 0} \\to \\R_{\\geq 0}\\] is the volume measure of neighbourhoods of $W_0$\\[\\begin{aligned} V(t) = \\int_{K(w) &amp;lt; t} \\varphi(w) dw. \\end{aligned}\\] From resolution of singularityHironaka’s resolution of singularity for the real analytic function $K(w)$ gives us a proper birational map2 $g: U \\to W$ such that in the neighbourhood of $w_0 \\in W_0$, the zero setof $K$\\[\\begin{aligned} K(g(u) - w_0) &amp;amp;= u^{2k} = u_1^{2k_1}u_2^{2k_2} \\dots u_d^{2k_d}\\\\ g&#39;(u) &amp;amp;= b(u)u^h = b(u)u_1^{h_1}u_2^{h_2} \\dots u_d^{h_d}\\end{aligned}\\] for some $u, k \\in \\N^d$ and analytic $b(u) \\neq 0$. We then have\\[\\begin{aligned} \\lambda = \\inf_{w \\in W_0} \\min_{1 \\leq j \\leq d}\\frac{h_j + 1}{2k_j}\\end{aligned}\\] and $\\theta$ is given by the number of times the above minimum is achieved.3 RLCT of ideals of analytic functionsTODO: there are various square roots involved in this that I don’treally understand RLCT for Regular ModelsThe RLCT of a regular realisable model is given by\\[\\lambda = \\frac{d}{2} \\quad \\theta = 1.\\]We shall use the Laplace integral characterisation of RLCT. We want toshow that\\[Z^0_n = \\int_W \\exp\\left(-nK(w)\\right) \\varphi(w) d{w} \\sim C n^{-\\frac{d}{2}}\\]for some positive constant \\(C\\) as \\(n \\to \\infty\\). Since the model is realisable and identifiable, it has unique minimum at \\(w^* \\in \\mathrm{supp}(\\varphi)\\). Taylor expansion of \\(K\\) centered around \\(w^*\\) up to order 2 gives\\[\\begin{aligned} K(w) &amp;amp;= K(w^*) + \\nabla K(w^*) \\cdot (w - w^*) + \\frac{1}{2} (w - w^*)^T \\nabla^2K(w^*)(w - w^*) + O(\\left|\\, w - w^* \\,\\right|^3) \\end{aligned}\\]where $\\nabla^2K(w^)$ is the Hessian of $K$ at $w^$. That $w^$ realises the true model and is a local minimum gives us $K(w^) = 0$ and $\\nabla K(w^*) =0$, reducing the above to\\[\\begin{aligned} K(w) &amp;amp;= \\frac{1}{2} (w - w^*)^T \\nabla^2K(w^*)(w - w^*) + O(\\left|\\, w - w^* \\,\\right|^3). \\end{aligned}\\]Substituting the above into the integral, we get, in the limit as \\(n \\to \\infty\\)\\[\\begin{aligned} Z^0_n &amp;amp;\\sim \\int_W \\exp\\left(-\\frac{n}{2} (w - w^*)^T \\nabla^2K(w^*)(w - w^*) \\right) \\varphi(w) d{w} \\end{aligned}\\]which we recognise as a Gaussian integral with precision matrix \\(n \\nabla^2K(w^*)\\) which is positive definite by assumption. Therefore, we conclude that\\[\\begin{aligned} Z^0_n \\sim \\varphi(w^*)\\sqrt{\\frac{(2\\pi)^d}{\\det\\left(n \\nabla^2K(w^*)\\right)}} = \\varphi(w^*)\\sqrt{\\frac{(2\\pi)^d}{\\det\\left(\\nabla^2K(w^*)\\right)}} n^{-\\frac{d}{2}}.\\end{aligned}\\]We shall use the characterisation that for any positive \\(a \\neq 1\\), \\(\\lambda = \\lim_{t \\to 0^+} \\log_a \\frac{V(at)}{V(t)}\\) where \\(V\\) is the volume function\\[\\begin{aligned} V(t) = \\int_{K(w) \\leq t} \\varphi(w) dw. \\end{aligned}\\]By regularity assumption, we have that \\(w^*\\) is a non-degenerate critical point of \\(K\\) and hence by Morse lemma, there is a local chart \\(x(w) = (x_i(w))_{i = 1, \\dots, d}\\) in a small enough neighbourhood of \\(w^*\\) such that \\(K(w) = \\cancelto{0}{K(w^*)} + \\sum_i x_i(w)^2\\). Therefore, for small enough \\(t &amp;gt; 0\\),\\[\\begin{aligned} V(t) = \\int_{\\sum_i x_i^2 \\leq t} \\varphi(x) dx\\end{aligned}\\]which is proportional to the volume of a \\(d\\)-dimensional ball with radius \\(\\sqrt{t}\\), i.e.\\(V(t) \\propto t^{d}\\).[^16] Finally,\\[\\begin{aligned} \\lambda = \\lim_{t \\to 0^+} \\log_a \\frac{(at)^{d/2}}{t^{d/2}} = \\frac{d}{2}. \\end{aligned}\\]Footnotes TODO: more examples. Contrast different inference tasks. &amp;#8617; obtained via recursive blow up. &amp;#8617; This deep result shows that\\((\\lambda, \\theta) \\in \\Q \\times \\Z\\). &amp;#8617; " } ]
